{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-rkEm89l0Nas"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaYyfFUqUnGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl numpy matplotlib spacy torchtext \n",
        "#!pip install torchtext==0.2.3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LTc4HW7UgQI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standard PyTorch imports\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# For plots\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AC8KeDJUgQO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base model for this and many \n",
        "    other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "        \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        memory = self.encoder(self.src_embed(src), src_mask)\n",
        "        output = self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
        "        return output\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yy7pY85UgQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psiq5idJUgQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"Core encoder is a stack of N layers\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEz9kLClUgQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx9JBwAcUgQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity we apply the norm first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer function that maintains the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mEBw9tIUgQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of two sublayers, self-attn and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3o_ZB42sUgQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMm6xHWVUgQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made up of three sublayers, self-attn, src-attn, and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        " \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyQKI9AgUgQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g7jaUXqq_vnc",
        "colab": {}
      },
      "source": [
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPJDCLbY0r43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"Implements Figure 2\"\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
        "        query, key, value = \\\n",
        "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch. \n",
        "        x, self.attn = attention(query, key, value, mask=mask, \n",
        "                                 dropout=self.dropout)\n",
        "        \n",
        "        # 3) \"Concat\" using a view and apply a final linear. \n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "             .view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta1iQnQH00ZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46lyhpKt04UM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQBrRM5c09TV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0.0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], \n",
        "                         requires_grad=False)\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgQMtvM-UgQl",
        "colab_type": "code",
        "outputId": "4dcf4427-58bd-4bb0-d781-aacf155c4874",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "source": [
        "# The attention mask shows the position each tgt word (row) is allowed to look at (column).\n",
        "# Words are blocked for attending to future words during training. \n",
        "plt.figure(figsize=(5,5))\n",
        "plt.imshow(subsequent_mask(20)[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff46d1e57f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATsAAAEvCAYAAAA6m2ZKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAASrklEQVR4nO3df6xcZZ3H8fdnC7hZJAKlIpQi6hISNAuSm6q7aHBxoTSEqmHdNmZFZVNxJZFkNwbXBI37z7pGTVyMpEoDGhbJqmizFqGLJmgiPwopUORXJRhasEXqgoguW/zuH/fUnd7OtLczc29v+7xfyWTOPM8z53x7Zu6Hc2bmPKSqkKSD3R/t7wIkaTYYdpKaYNhJaoJhJ6kJhp2kJhh2kppwyP4uoJ9jjp5XJy06dJ+f98h9fzID1Ug6UPyO3/Bi/U/69c3JsDtp0aHcefOifX7eucefPgPVSDpQ3FG3DuzzNFZSE0YKuyRLkjycZFOSy/v0vyzJDV3/HUlOGmV7kjSsocMuyTzgS8B5wKnAiiSnThl2MfCrqvpT4AvAZ4bdniSNYpQju8XApqp6rKpeBL4BLJsyZhlwbbf8TeDsJH0/PJSkmTRK2C0Enuh5vLlr6zumqnYAzwLzR9imJA1lznxBkWRlkvVJ1j/9zEv7uxxJB5lRwm4L0Pv7kBO6tr5jkhwCvAJ4pt/KqmpVVU1U1cSC+fNGKEuSdjdK2N0FnJzkNUkOA5YDa6aMWQNc1C1fCPygnEBP0n4w9I+Kq2pHkkuBm4F5wOqqeiDJp4H1VbUGuBr4epJNwHYmA1GSZt1IV1BU1Vpg7ZS2K3qWfwf89SjbkKRxmDNfUEjSTDLsJDVhTk4EMKybn9ywz89x8gCpDR7ZSWqCYSepCYadpCYYdpKaYNhJaoJhJ6kJhp2kJhh2kppg2ElqgmEnqQmGnaQmGHaSmnBQTQQwjGEmDwAnEJAONB7ZSWqCYSepCYadpCYYdpKaYNhJaoJhJ6kJhp2kJhh2kppg2ElqwtBhl2RRkh8m+WmSB5J8tM+Ys5I8m2RDd7titHIlaTijXC62A/iHqronyRHA3UnWVdVPp4z7UVWdP8J2JGlkQx/ZVdVTVXVPt/xr4EFg4bgKk6RxGstndklOAt4I3NGn+y1J7k1yU5LXj2N7krSvRp71JMnLgW8Bl1XVc1O67wFeXVXPJ1kKfAc4ecB6VgIrAU5cOPcnYxlmthRnSpH2n5GO7JIcymTQXVdV357aX1XPVdXz3fJa4NAkx/RbV1WtqqqJqppYMH/eKGVJ0m5G+TY2wNXAg1X1+QFjXtWNI8nibnvPDLtNSRrWKOeLfwH8LXB/kp3ndP8EnAhQVVcBFwIfTrID+C2wvKpqhG1K0lCGDruq+jGQvYy5Erhy2G1I0rh4BYWkJhh2kppg2ElqgmEnqQmGnaQmGHaSmmDYSWqCYSepCXP/ivuDyDCTB4ATCEjj4JGdpCYYdpKaYNhJaoJhJ6kJhp2kJhh2kppg2ElqgmEnqQmGnaQmGHaSmmDYSWqCYSepCYadpCY468kBwNlSpNF5ZCepCYadpCaMHHZJHk9yf5INSdb36U+SLybZlOS+JGeMuk1J2lfj+szu7VX1ywF95wEnd7c3AV/u7iVp1szGaewy4Gs16XbgyCTHzcJ2JekPxhF2BdyS5O4kK/v0LwSe6Hm8uWuTpFkzjtPYM6tqS5JXAuuSPFRVt+3rSrqgXAlw4kJ/ESNpvEY+squqLd39NuBGYPGUIVuART2PT+japq5nVVVNVNXEgvnzRi1LknYxUtglOTzJETuXgXOAjVOGrQHe130r+2bg2ap6apTtStK+GvV88VjgxiQ71/XvVfX9JJcAVNVVwFpgKbAJeAH4wIjblKR9NlLYVdVjwGl92q/qWS7gI6NsR5JG5RUUkppg2Elqgr/xOIgNM1uKM6XoYOWRnaQmGHaSmmDYSWqCYSepCYadpCYYdpKaYNhJaoJhJ6kJhp2kJhh2kppg2ElqgmEnqQlOBKBdDDN5ADiBgOY+j+wkNcGwk9QEw05SEww7SU0w7CQ1wbCT1ATDTlITDDtJTTDsJDVh6LBLckqSDT2355JcNmXMWUme7RlzxeglS9K+G/pysap6GDgdIMk8YAtwY5+hP6qq84fdjiSNw7hOY88GflZVPx/T+iRprMYVdsuB6wf0vSXJvUluSvL6MW1PkvbJyLOeJDkMuAD4eJ/ue4BXV9XzSZYC3wFOHrCelcBKgBMXOhnLgWaY2VKcKUWzaRxHducB91TV1qkdVfVcVT3fLa8FDk1yTL+VVNWqqpqoqokF8+eNoSxJ+n/jCLsVDDiFTfKqJOmWF3fbe2YM25SkfTLS+WKSw4G/Aj7U03YJQFVdBVwIfDjJDuC3wPKqqlG2KUnDGCnsquo3wPwpbVf1LF8JXDnKNiRpHLyCQlITDDtJTTDsJDXBsJPUBMNOUhMMO0lNMOwkNcGwk9QEr7jXfjPM5AHgBAIajkd2kppg2ElqgmEnqQmGnaQmGHaSmmDYSWqCYSepCYadpCYYdpKaYNhJaoJhJ6kJhp2kJhh2kprgrCc64DhbiobhkZ2kJhh2kpowrbBLsjrJtiQbe9qOTrIuyaPd/VEDnntRN+bRJBeNq3BJ2hfTPbK7Blgype1y4NaqOhm4tXu8iyRHA58E3gQsBj45KBQlaSZNK+yq6jZg+5TmZcC13fK1wDv7PPVcYF1Vba+qXwHr2D00JWnGjfKZ3bFV9VS3/Avg2D5jFgJP9Dze3LVJ0qwayxcUVVVAjbKOJCuTrE+y/ulnXhpHWZL0B6OE3dYkxwF099v6jNkCLOp5fELXtpuqWlVVE1U1sWD+vBHKkqTdjRJ2a4Cd365eBHy3z5ibgXOSHNV9MXFO1yZJs2q6Pz25HvgJcEqSzUkuBv4F+KskjwLv6B6TZCLJVwGqajvwz8Bd3e3TXZskzappXS5WVSsGdJ3dZ+x64O96Hq8GVg9VnSSNiVdQSGqCYSepCc56omYMM1uKM6UcPDyyk9QEw05SEww7SU0w7CQ1wbCT1ATDTlITDDtJTTDsJDXBsJPUBMNOUhMMO0lNMOwkNcGJAKQ9GGbyAHACgbnIIztJTTDsJDXBsJPUBMNOUhMMO0lNMOwkNcGwk9QEw05SEww7SU3Ya9glWZ1kW5KNPW2fTfJQkvuS3JjkyAHPfTzJ/Uk2JFk/zsIlaV9M58juGmDJlLZ1wBuq6s+AR4CP7+H5b6+q06tqYrgSJWl0ew27qroN2D6l7Zaq2tE9vB04YQZqk6SxGcdndh8EbhrQV8AtSe5OsnIM25KkoYw060mSTwA7gOsGDDmzqrYkeSWwLslD3ZFiv3WtBFYCnLjQyVh0YBtmthRnSplZQx/ZJXk/cD7w3qqqfmOqakt3vw24EVg8aH1VtaqqJqpqYsH8ecOWJUl9DRV2SZYAHwMuqKoXBow5PMkRO5eBc4CN/cZK0kybzk9Prgd+ApySZHOSi4ErgSOYPDXdkOSqbuzxSdZ2Tz0W+HGSe4E7ge9V1fdn5F8hSXux1w/HqmpFn+arB4x9EljaLT8GnDZSdZI0Jl5BIakJhp2kJhh2kppg2ElqgmEnqQmGnaQmGHaSmmDYSWqCV9xLc8QwkweAEwhMl0d2kppg2ElqgmEnqQmGnaQmGHaSmmDYSWqCYSepCYadpCYYdpKaYNhJaoJhJ6kJhp2kJhh2kprgrCfSAc7ZUqbHIztJTTDsJDVhr2GXZHWSbUk29rR9KsmWJBu629IBz12S5OEkm5JcPs7CJWlfTOfI7hpgSZ/2L1TV6d1t7dTOJPOALwHnAacCK5KcOkqxkjSsvYZdVd0GbB9i3YuBTVX1WFW9CHwDWDbEeiRpZKN8Zndpkvu609yj+vQvBJ7oeby5a5OkWTds2H0ZeB1wOvAU8LlRC0myMsn6JOuffualUVcnSbsYKuyqamtVvVRVvwe+wuQp61RbgEU9j0/o2gatc1VVTVTVxIL584YpS5IGGirskhzX8/BdwMY+w+4CTk7ymiSHAcuBNcNsT5JGtdcrKJJcD5wFHJNkM/BJ4KwkpwMFPA58qBt7PPDVqlpaVTuSXArcDMwDVlfVAzPyr5Ckvdhr2FXVij7NVw8Y+ySwtOfxWmC3n6VI0mzzCgpJTTDsJDXBWU+kRg0zW8qBPFOKR3aSmmDYSWqCYSepCYadpCYYdpKaYNhJaoJhJ6kJhp2kJhh2kppg2ElqgmEnqQmGnaQmOBGApGkbZvIAmBsTCHhkJ6kJhp2kJhh2kppg2ElqgmEnqQmGnaQmGHaSmmDYSWqCYSepCXu9giLJauB8YFtVvaFruwE4pRtyJPDfVbXbT6STPA78GngJ2FFVE2OqW5L2yXQuF7sGuBL42s6GqvqbnctJPgc8u4fnv72qfjlsgZI0DnsNu6q6LclJ/fqSBHgP8JfjLUuSxmvUz+zeCmytqkcH9BdwS5K7k6wccVuSNLRRZz1ZAVy/h/4zq2pLklcC65I8VFW39RvYheFKgBMXOhmLdDAZZraUcc+UMvSRXZJDgHcDNwwaU1VbuvttwI3A4j2MXVVVE1U1sWD+vGHLkqS+RjmNfQfwUFVt7teZ5PAkR+xcBs4BNo6wPUka2l7DLsn1wE+AU5JsTnJx17WcKaewSY5PsrZ7eCzw4yT3AncC36uq74+vdEmavul8G7tiQPv7+7Q9CSztlh8DThuxPkkaC6+gkNQEw05SEww7SU0w7CQ1wbCT1ATDTlITDDtJTTDsJDXBK+4lzUnDTB6w+NwXBvZ5ZCepCYadpCYYdpKaYNhJaoJhJ6kJhp2kJhh2kppg2ElqgmEnqQmGnaQmGHaSmmDYSWqCYSepCamq/V3DbpI8Dfy8T9cxwC9nuZx+rGNX1rEr69jVbNbx6qpa0K9jTobdIEnWV9WEdViHdVjHvvI0VlITDDtJTTjQwm7V/i6gYx27so5dWceu5kQdB9RndpI0rAPtyE6ShjInwy7JkiQPJ9mU5PI+/S9LckPXf0eSk2aghkVJfpjkp0keSPLRPmPOSvJskg3d7Ypx19Ft5/Ek93fbWN+nP0m+2O2P+5KcMQM1nNLz79yQ5Lkkl00ZMyP7I8nqJNuSbOxpOzrJuiSPdvdHDXjuRd2YR5NcNAN1fDbJQ91+vzHJkQOeu8fXcAx1fCrJlp59v3TAc/f4tzWGOm7oqeHxJH3/rznj3B/TVlVz6gbMA34GvBY4DLgXOHXKmL8HruqWlwM3zEAdxwFndMtHAI/0qeMs4D9nYZ88Dhyzh/6lwE1AgDcDd8zCa/QLJn/TNOP7A3gbcAawsaftX4HLu+XLgc/0ed7RwGPd/VHd8lFjruMc4JBu+TP96pjOaziGOj4F/OM0Xrc9/m2NWseU/s8BV8z0/pjubS4e2S0GNlXVY1X1IvANYNmUMcuAa7vlbwJnJ8k4i6iqp6rqnm7518CDwMJxbmOMlgFfq0m3A0cmOW4Gt3c28LOq6vfD77GrqtuA7VOae98D1wLv7PPUc4F1VbW9qn4FrAOWjLOOqrqlqnZ0D28HThh2/aPUMU3T+dsaSx3d3+N7gOuHXf+4zcWwWwg80fN4M7uHzB/GdG+0Z4H5M1VQd5r8RuCOPt1vSXJvkpuSvH6GSijgliR3J1nZp386+2ycljP4TTwb+wPg2Kp6qlv+BXBsnzGzvV8+yOQRdj97ew3H4dLudHr1gNP62dwfbwW2VtWjA/pnY3/sYi6G3ZyS5OXAt4DLquq5Kd33MHkqdxrwb8B3ZqiMM6vqDOA84CNJ3jZD29mrJIcBFwD/0ad7tvbHLmryvGi//qwgySeAHcB1A4bM9Gv4ZeB1wOnAU0yeQu5PK9jzUd2sv6fnYthtARb1PD6ha+s7JskhwCuAZ8ZdSJJDmQy666rq21P7q+q5qnq+W14LHJrkmHHXUVVbuvttwI1Mno70ms4+G5fzgHuqamufOmdlf3S27jxV7+639RkzK/slyfuB84H3dsG7m2m8hiOpqq1V9VJV/R74yoD1z9b+OAR4N3DDoDEzvT/6mYthdxdwcpLXdEcRy4E1U8asAXZ+s3Yh8INBb7JhdZ85XA08WFWfHzDmVTs/K0yymMn9OdbQTXJ4kiN2LjP5gfjGKcPWAO/rvpV9M/BszyneuA38L/Zs7I8eve+Bi4Dv9hlzM3BOkqO607pzuraxSbIE+BhwQVW9MGDMdF7DUevo/Yz2XQPWP52/rXF4B/BQVW3u1zkb+6Ov2fw2ZLo3Jr9dfITJb44+0bV9msk3FMAfM3katQm4E3jtDNRwJpOnRvcBG7rbUuAS4JJuzKXAA0x+q3U78OczUMdru/Xf221r5/7orSPAl7r9dT8wMUOvy+FMhtcretpmfH8wGa5PAf/L5OdMFzP5Ge2twKPAfwFHd2MngK/2PPeD3ftkE/CBGahjE5Ofg+18j+z8lcDxwNo9vYZjruPr3Wt/H5MBdtzUOgb9bY2zjq79mp3viZ6xM7Y/pnvzCgpJTZiLp7GSNHaGnaQmGHaSmmDYSWqCYSepCYadpCYYdpKaYNhJasL/AXkopxz7l3OuAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heaKRIaZUgQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    \"Standard generation step. (Not described in the paper.)\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-LDhRoaUgQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "    \"Construct a model object based on hyperparameters.\"\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(h, d_model, dropout)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
        "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        Generator(d_model, tgt_vocab))\n",
        "    \n",
        "    # This was important from their code. Initialize parameters with Glorot or fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform(p)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP-g4KfhUgQ_",
        "colab_type": "code",
        "outputId": "6caf9bea-e4e7-416e-9489-c8d5b94e46aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Small example model.\n",
        "tmp_model = make_model(10, 10, 2)\n",
        "tmp_model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EncoderDecoder(\n",
              "  (encoder): Encoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm(\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (src_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (src_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm(\n",
              "    )\n",
              "  )\n",
              "  (src_embed): Sequential(\n",
              "    (0): Embeddings(\n",
              "      (lut): Embedding(10, 512)\n",
              "    )\n",
              "    (1): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "  )\n",
              "  (tgt_embed): Sequential(\n",
              "    (0): Embeddings(\n",
              "      (lut): Embedding(10, 512)\n",
              "    )\n",
              "    (1): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "  )\n",
              "  (generator): Generator(\n",
              "    (proj): Linear(in_features=512, out_features=10)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5yV0f2QUgRL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note: This part is incredibly important. \n",
        "# Need to train with this setup of the model is very unstable.\n",
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup**(-1.5)))\n",
        "        \n",
        "def get_std_opt(model):\n",
        "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM5n1PJxUgRN",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PC2wCVEFUgRO",
        "colab_type": "code",
        "outputId": "644444f0-719e-4a8e-bba5-c8746383a469",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "# Three settings of the lrate hyperparameters.\n",
        "opts = [NoamOpt(512, 1, 4000, None), \n",
        "        NoamOpt(512, 1, 8000, None),\n",
        "        NoamOpt(256, 1, 4000, None)]\n",
        "plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\n",
        "plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])\n",
        "None"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD4CAYAAAAHHSreAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzdd1hUV/rA8e+hV+l1AKmKCIgI9m7sLSZxNcmapkk22SSburvZ3fRsNuWXtptqYk8xJlFjojEmMjbELvYuyABKL9LLnN8fQ4hEFERgBjif5+HJcO+5Z94heF/uPee+R0gpURRFUZRLmRk7AEVRFMX0qOSgKIqiXEYlB0VRFOUyKjkoiqIol1HJQVEURbmMhbEDaA3u7u4yMDDQ2GEoiqJ0KHv37s2VUno0tq9TJIfAwED27Nlj7DAURVE6FCHEuSvtU7eVFEVRlMuo5KAoiqJcRiUHRVEU5TLNGnMQQkwA3gXMgU+llK/+br81sBToB+QBs6SUqXX7ngbmArXAI1LKn+q2LwSmANlSyshL+nIFvgICgVTgD1LKghZ/QkVROqTq6mrS09OpqKgwdigdno2NDX5+flhaWjb7mCaTgxDCHHgfGAukA7uFEGuklEcvaTYXKJBShgohZgOvAbOEEBHAbKA34Av8IoToIaWsBRYD72FIKpf6O7BRSvmqEOLvdd//rdmfSFGUTiE9PR1HR0cCAwMRQhg7nA5LSkleXh7p6ekEBQU1+7jm3FbqD5yWUp6VUlYBy4Hpv2szHVhS9/obYIww/N+cDiyXUlZKKVOA03X9IaXcAuQ38n6X9rUEuLHZn0ZRlE6joqICNzc3lRiukxACNze3a74Ca05y0AC6S75Pr9vWaBspZQ1QBLg189jf85JSnq97fQHwaqyREOI+IcQeIcSenJycZnwMRVE6GpUYWkdLfo4mPSAtDfXEG60pLqWcL6WMk1LGeXg0+gyH0ojtGds5mne06YaKonRpzUkOGYD/Jd/71W1rtI0QwgJwwjAw3Zxjfy9LCOFT15cPkN2MGJVmqNZXc/8v9zPrh1kUVxUbOxxF6RACAwOJiooiJiaGuLg4AL7++mt69+6NmZlZgwdwf/75Z/r160dUVBT9+vUjISHhqn2/+eabCCHIzc0FDOMDjzzyCKGhoURHR7Nv3776tkuWLCEsLIywsDCWLFlSv33v3r1ERUURGhrKI488Qmut0dOc5LAbCBNCBAkhrDAMMK/5XZs1wJ11r28BEur+6l8DzBZCWAshgoAwYFcT73dpX3cC3zUjRqUZ9lz47Zf4rT1vGTESRelYtFotycnJ9YkgMjKSlStXMnz48Abt3N3d+f777zl06BBLlixhzpw5V+xTp9OxYcMGAgIC6rf9+OOPnDp1ilOnTjF//nweeOABAPLz83nhhRfYuXMnu3bt4oUXXqCgwDCJ84EHHuCTTz6pP279+vWt8pmbTA51YwgPAT8Bx4AVUsojQogXhRDT6potANyEEKeBxzHMMEJKeQRYARwF1gN/rpuphBDiSyAJ6CmESBdCzK3r61VgrBDiFHBD3fdKK9DqtNiY2zC752y+PfUtuy/sNnZIitIh9erVi549e162vW/fvvj6+gLQu3dvysvLqaysbLSPxx57jNdff73BeMB3333HHXfcgRCCgQMHUlhYyPnz5/npp58YO3Ysrq6uuLi4MHbsWNavX8/58+cpLi5m4MCBCCG44447WL16dat8xmY95yClXAes+922Zy95XQHMvMKx/wb+3cj2W6/QPg8Y05y4lOaTUqLVaRnoO5DH4x5nW8Y2nt/+PN9O+xYbCxtjh6coV/XC90c4mtm6t0IjfLvx3NTeTbYTQjBu3DiEENx///3cd999zer/22+/JTY2FmtrawDmzZvHn/70J+Li4vjuu+/QaDT06dOnwTEZGRn4+/92J97Pz4+MjIyrbvfz87tse2voFIX3lKYdzz/OhdILPNjnQWwtbHlu8HPcu+FePjrwEY/2e9TY4SmKydq2bRsajYbs7GzGjh1LeHj4ZbeTfu/IkSP87W9/Y8OGDfXbPv30UwDKysp45ZVXGuwzRSo5dBFanRaBYLif4Zd6oM9AZoTOYPGRxYwJGEOUR5SRI1SUK2vOX/htRaMxzL739PRkxowZ7Nq166rJIT09nRkzZrB06VJCQkIu23/mzBlSUlLqrxrS09OJjY1l165daDQadDpdg740Gg0ajYZNmzY12D5y5Eg0Gg3p6emXtW8NJj2VVWk9Wp2WGM8Y3Gzd6rc9Gf8k7rbu/GPbPyivKTdidIpimkpLS7l48WL96w0bNhAZGXnF9oWFhUyePJlXX32VIUOGNNomKiqK7OxsUlNTSU1Nxc/Pj3379uHt7c20adNYunQpUkp27NiBk5MTPj4+jB8/ng0bNlBQUEBBQQEbNmxg/Pjx+Pj40K1bN3bs2IGUkqVLlzJ9+u+fUW4ZlRy6gPMl5zmef5xR/qMabO9m1Y1/D/03qcWpavaSojQiKyuLoUOH0qdPH/r378/kyZOZMGECq1atws/Pj6SkJCZPnsz48eMBeO+99zh9+jQvvvgiMTExxMTEkJ1tmI0/b968JtedmTRpEsHBwYSGhnLvvffywQcfAODq6sozzzxDfHw88fHxPPvss7i6ugLwwQcfMG/ePEJDQwkJCWHixImt8tlFa82JNaa4uDipFvu5si+OfcF/dv2H72/8nkCnwMv2v7H7DZYeXcqHN3zIUM3Q9g9QURpx7NgxevXqZewwOo3Gfp5CiL1SyrjG2qsrhy5Aq9MS2C2w0cQA8EjsI4Q6h/Js4rMUVhS2b3CKopgklRw6ueKqYvZc2MOogFFXbGNtbs1/hv2HgsoCnt3+bKs9YakoSselkkMnty19GzWyhtH+o6/aLtw1nMf7PY5Wp2XZ0WXtFJ2iKKZKJYdOTqvT4mrjSpR701NV/9jrj4z2H83be9/mYM7BdohOURRTpZJDJ1ZdW822jG2M9B+JuZl5k+2FELw45EW87L14avNTFFUWtUOUiqKYIpUcOrHdWbspqS65bArr1ThZO/F/I/6P7PJs/pX4LzX+oChdlEoOnZg2zVBob4DPgGs6LtI9kifjnmSTbhPzD85vo+gUpWNoi5LdycnJDBw4sL7PXbsMxapNqWQ3UsoO/9WvXz+pNKTX6+UNX98gH974cIuPf3rL0zJycaTceG5jK0enKE07evSosUOQUkrZvXt3mZOT02Db0aNH5fHjx+WIESPk7t2767fv27dPZmRkSCmlPHTokPT19W20z7Fjx8p169ZJKaVcu3atHDFiRP3rCRMmSL1eL5OSkmT//v2llFLm5eXJoKAgmZeXJ/Pz82VQUJDMz8+XUkoZHx8vk5KSpF6vlxMmTKjv9/ca+3kCe+QVzqvqyqGTOpZ/jAulF67pltKlhBA8O+hZIt0ieXrr05wuON3KESpKx3W9JbuFEBQXG6rMFhUV1R/T4Up2Kx2PVqfFTJgxwn9Ei/uwsbDh7VFvM/uH2fxF+xe+mPwFTtZOrRilojTTj3+HC4dat0/vKJjY9HIxbVGy+5133mH8+PE8+eST6PV6tm/fDphWyW515dBJbdJtIsYjBlcb1+vqx9vem3dGvUNmaSZPbX6Kan11K0WoKB3Dtm3b2LdvHz/++CPvv/8+W7ZsafKYX0t2f/zxx/XbPv300/oxiw8//JC3334bnU7H22+/zdy5c6/UldGoK4dOKLMkk+P5x3mi3xOt0l+MZwzPDnyWZ7c/y0tJL/HC4BcarF6lKG2uGX/ht5XWLtkNhsHld999F4CZM2cyb968+vdSJbuVNqPVaQEY6T+y1fqcETaDP/X5E6tOr+Ljgx83fYCidAJtUbIbwNfXl82bNwOQkJBAWFgYgEmV7Db6TKPW+FKzlRqa+9NcOXXV1FbvV6/Xy39s/YeMXBwpV59a3er9K8qlTGG20pkzZ2R0dLSMjo6WERER8uWXX5ZSSrly5Uqp0WiklZWV9PT0lOPGjZNSSvnSSy9JOzs72adPn/qvrKwsKaWUc+fOrZ/ZtHXrVhkbGyujo6Nl//795Z49e6SUhn9jDz74oAwODpaRkZENZkItWLBAhoSEyJCQELlw4cL67bt375a9e/eWwcHB8s9//rPU6/WNfpZrna2kSnZ3MsVVxYxYPoI7et/BY/0ea/X+q2ureXDjg+y5sIf3b3ifwb6DW/09FAVUye7Wpkp2d3Fb07dSI2taPIW1KZbmlrw18i2CnIN4VPsoB3IOtMn7KIpiXCo5dDKbdJtws3Ej2iO6zd7D0cqRj2/4GHdbdx745QFO5J9os/dSFMU4VHLoRC4ttGcm2vZ/rYedB5+M+wQ7Czvu//l+UotS2/T9FEVpXyo5dCK7L1x7ob3roXHQMH/cfCSSe3++l/Ml59vlfRVFaXsqOXQiCboEbC1sr7nQ3vUIdgrm47EfU1pVytwNc7lQeqHd3ltRlLajkkMnIaVkk24Tg3wGYWNh067vHe4azkdjP6KgooC71t9FRknrPL6vKIrxqOTQSRzNP0pWWdZV14puS9Ee0Xw67lOKq4q5e/3d6C7qmj5IUUycTqdj1KhRRERE0Lt37/qnmp9//nk0Gg0xMTHExMSwbt26+mMOHjzIoEGD6N27N1FRUVRUVFyx/zfffBMhBLm5uYAq2a0egmsD/9v3Pxm9JFrml+cbNY4juUfkkC+HyDErxshzReeMGovSsZnCQ3CZmZly7969Ukopi4uLZVhYmDxy5Ih87rnn5BtvvHFZ++rqahkVFSWTk5OllFLm5ubKmpqaRvtOS0uT48aNkwEBAfUlwVXJbqXV/Vpoz8XGxahxRLhFsGDcAqpqq7h7/d2q1LfSofn4+BAbGwuAo6MjvXr1umrV0w0bNhAdHU2fPn0AcHNzw9y88SV6H3vsMV5//fUGdcpUyW6lVWWUZHCi4ARPxj1p7FAA6OnakwXjF3D/z/dzx/o7eH/M+/T17GvssJQO7LVdr3E8/3ir9hnuGs7f+v+t2e1TU1PZv38/AwYMIDExkffee4+lS5cSFxfHm2++iYuLCydPnkQIwfjx48nJyWH27Nn89a9/BRqW7P7uu+/QaDT1SeRXqmS30qo26TYBrVto73qFuYSxbNIyXG1cuXfDvWzWbTZ2SIrSYiUlJdx888288847dOvWjQceeIAzZ86QnJyMj48PTzxhqIBcU1PDtm3b+Pzzz9m2bRurVq1i48aNwG8lu8vKynjllVd48cUXjfmRmtSsKwchxATgXcAc+FRK+erv9lsDS4F+QB4wS0qZWrfvaWAuUAs8IqX86Wp9CiHGAG9gSFwlwF1SSnVv4iq0aVqCnYLp3q27sUNpQOOgYcmEJTy48UH+ov0Lzw9+nhtDbzR2WEoHdC1/4be26upqbr75Zm6//XZuuukmALy8vOr333vvvUyZMgUw/OU+fPhw3N3dAZg0aRL79u1jzJgx9e3PnDlDSkpK/VVDeno6sbGx7Nq1y6RKdjc52Ivh5H0GCAasgANAxO/aPAh8VPd6NvBV3euIuvbWQFBdP+ZX6xM4CfS6pN/FTcXYlQekCysKZZ8lfeTbe942dihXVFJVIuf9NE9GLo6U8w/Mv2LVSEW5lCkMSOv1ejlnzhz5l7/8pcH2zMzM+tdvvfWWnDVrlpRSyvz8fNm3b19ZWloqq6ur5ZgxY+QPP/xw1fe4dI3qH374ocGAdHx8vJTSMCAdGBgo8/PzZX5+vgwMDJR5eXlSyssHpNeuXdvo+1zrgHRzrhz6A6ellGcBhBDLgenA0UvaTAeer3v9DfCeMIyyTAeWSykrgRQhxOm6/rhKnxLoVtfGCchsRoxd1taMrdTKWqNNYW0Oe0t73h/zPs8kPsN/9/+X1OJUnhv0HFbmVsYOTVGuKjExkWXLlhEVFUVMTAwAr7zyCl9++SXJyckIIQgMDKxf8c3FxYXHH3+c+Ph4hBBMmjSJyZMnAw3HHK5k0qRJrFu3jtDQUOzs7Fi0aBEArq6uPPPMM8THxwPw7LPP4upqWOXxgw8+4K677qK8vJyJEycyceLEVvnsTZbsFkLcAkyQUs6r+34OMEBK+dAlbQ7XtUmv+/4MMABDwtghpfysbvsC4Me6wxrtUwgxDFgNlAPFwEApZXEjcd0H3AcQEBDQ79y5cy37CXRwT25+kr1Ze9k4c2Ob11O6XlJKPjr4ER8kf0CsZyzvjHrH6LOrFNOlSna3rs5QsvsxYJKU0g9YBLzVWCMp5XwpZZyUMs7Dw6NdAzQVVbVVbMvYxgi/ESafGMCwUPsDfR7g9eGvczj3MLevu52zRWeNHZaiKI1ozhklA/C/5Hu/um2NthFCWGC4HZR3lWMb3S6E8AD6SCl31m3/ClCryVzB7gu7Ka0ubbdCe61lYtBEFk5YSGl1KX9c+0e2pDe9YLuiKO2rOclhNxAmhAgSQlhhGHBe87s2a4A7617fAiTUDXasAWYLIayFEEFAGLDrKn0WAE5CiB51fY0FjrX843VuWp223QvttZY+Hn34YvIXaBw1PLTxIT5M/hC91Bs7LMXENHXbW2melvwcm0wOUsoa4CHgJwwn6hVSyiNCiBeFENPqmi0A3OoGnB8H/l537BFgBYaB5vXAn6WUtVfqs277vcC3QogDwBzgqWv+VF2AlBKtTstg38HtXmivtWgcNCybuIypIVP54MAHPJzwMEWVRcYOSzERNjY25OXlqQRxnaSU5OXlYWNzbecJtYZ0B3Uk9wiz187m5SEvMz10urHDuS5SSlacWMGru1/F286bd0a9Q0/XnsYOSzGy6upq0tPTr1q4TmkeGxsb/Pz8sLS0bLD9agPSqnxGB6XVaTETZgz3G27sUK6bEIJZ4bPo6dqTJzY9we3rbuev8X9lZo+ZDerOKF2LpaUlQUFBxg6jyzL9KS5Ko7Q6LX09+3aqqaAxnjF8NfUrYj1jeWnHSzyx+QmKqy6bxawoSjtQyaEDSr+YzsmCkx1ullJzuNu689HYj3is32No07TMXDOT5OxkY4elKF2OSg4d0K+F9jpjcgAwE2bcE3kPSyYuQQjBXevvYv7B+dTqa40dmqJ0GSo5dEBanZYQpxACugUYO5Q2Fe0RzddTv2Zc93H8b///uHP9naQWpRo7LEXpElRy6GCKKovYm7XXpGsptSZHK0deG/4arw17jZSiFGZ+P5PPj32unolQlDamkkMHU19or5PeUmqMEIJJwZNYNX0V8d7xvLrrVeZtmEdGSessaqIoyuVUcuhgtGlaPGw9iHSPNHYo7c7TzpP3x7zPi4Nf5GjeUW767ia+PP6lGotQlDagkkMHUl9oz79jFNprC0IIZoTNYOW0lfTx6MMrO1/hjvV3cLLgpLFDU5ROpWueYTqoXRd2UVZT1qVuKV2Jr4MvH4/9mFeGvoKuWMes72fxzt53qKhRT9MqSmtQyaED0aZ13EJ7bUEIwdSQqay5cQ2Tgyez4PACZnw3g+0Z240dmqJ0eCo5dBB6qWeTbhNDfIdgbW5t7HBMirONMy8PfZlPx32KuZk59/9yP49pH1MD1opyHVRy6CCO5h0luzy7y0xhbYkBPgP4dtq3PNz3YRIzE5m+ejofJH9AeU25sUNTlA5HJYcOQqvTYi7MGa65vkJ73x/IJPF0bitFZXqsza25L/o+1ty4htH+o/nwwIdMXz2dDakbVOlnRbkGKjl0EL8W2nO2cW5xH9nFFTz85X5u/3Qn20513gQB4G3vzesjXmfR+EU4WjnyxOYnmLthLkdyjxg7NEXpEFRy6ADSL6ZzquAUI/1HXlc/n+1MA8DG0oz7lu3hYHphK0Rn2uK84/hqylf8a8C/OFN4htlrZ/PU5qfQFeuMHZqimDSVHDoArU4LwGj/0S3uo6K6ls93nGNMuCebnxqFq70Vdy3azdmcktYK02RZmFkwK3wWa2es5f7o+9mcvplp303jPzv/Q35FvrHDUxSTpJJDB6DVaQl1DsW/m3+L+/j+QCZ5pVXcMzQIr242LL2nPwBzFuzifFHXGLB1sHLgob4PsXbGWm4MvZGvTnzFpJWT+PjAx5RWlxo7PEUxKSo5mLiiyiL2Ze27rgffpJQsTEylp5cjg0PcAAj2cGDx3fEUlVdz6/wdXCjqOg+Pedh58Nyg51g5fSUDvAfwXvJ7TPh2AgsOLaCsuszY4SmKSVDJwcRtSd9y3YX2dqbkc+x8MXcPCWyw7Ga0nzNL7ulPzsVKbvtkB1nFXSdBAAQ7BfPu6Hf5fNLn9HbvzTv73mHiyoksPrxYTX9VujyVHEycVmcotNfbvXeL+1i4LQUXO0tu7Ku5bF+/7i4suac/WcUV3PrJDrK7WIIAw7oRH93wEcsmLqOnS0/e3PsmE7+dyNIjS1U5DqXLUsnBhFXVVpGYkXhdhfbS8sr4+VgWtw0IwMbSvNE2cYGuLL6nPxeKDAmiq4xB/F6MZwzzx81n6cSlhLqE8saeNxj/7Xg+PfSpWsta6XJUcjBhO8/vvO5Ce0uSUjEXgjkDA6/aLj7QlcV39yeruJJbPkwiJbfrDtD29ezLp+M+ZdH4RfRy68W7+95l3DfjeGvvW+SU5Rg7PEVpFyo5mDCt7voK7ZVU1rBit45JUT54O9k02b5/kCvL7xtIRXUtMz/azpHMoha9b2cR5x3HRzd8xNdTv2a4ZjhLjixh/LfjeSHpBdKK04wdnqK0KZUcTNSvhfaGaoa2uNDeN3t0XKys4e4hgc0+JlLjxIo/DcLK3IzZH+9gV4p6DiDcNZzXR7zODzf+wIzQGaw5vYapq6fy+KbH2Ze1T5XlUDollRxM1NG8o+SU57T4lpJeL1m8PZW+Ac70DXC5pmNDPBz45oHBeHSz5o8LdvL9gcwWxdDZ+Hfz55lBz/DTLT9xd++72Xl+J3euv5NZP8xizZk1VNVWGTtERWk1KjmYqIS0BEOhPb+WFdrTnsgmNa+Mu4cEteh4X2dbvvnTYPr4OfHwl/t5L+GU+gu5jrutO4/2e5Sfb/mZZwY+Q2VtJf/c9k/GfTOOD5M/JLe8c9etUroGlRxM1K+F9pysnVp0/KLEVLy72TAx0rvFMbjaW/HZvAHM6Kvh/zac5MmvD1JVo29xf52NnaUdf+j5B1ZPX83HYz8mwi2CDw58wLhvxvGPrf8gOTtZJVSlw7IwdgDK5XQXdZwuPM1TcU+16PgTFy6y7XQuT43viaX59eV/awtz3vpDH7q72fHOL6dILyjj/dtjcXdQCw79SgjBYN/BDPYdTGpRKl8c/4I1Z9bw/dnvCXMJY2aPmUwJnoKjlaOxQ1WUZlNXDiZIm2YotNfShX0Wb0/B2sKM2/oHtEo8QggevaEH786OIVlXyNT/bSNZ1/krurZEoFMg/xjwDxJmJvDcoOewEBa8svMVxnw9hmcTn+Vw7mF1NaF0CCo5mKD6QnuO115oL7+0ipX7MrgpVoOLvVWrxjU9RsO3DwzG3Ezwh4+S+GJnmjrRXYGdpR239LiFFVNXsHzyciYFTWJ96npuXXsrs36YxVfHv6KosmtPFVZMW7OSgxBighDihBDitBDi743stxZCfFW3f6cQIvCSfU/XbT8hhBjfVJ/C4N9CiJNCiGNCiEeu7yN2LIUVhezP3t/iWUpf7kqjskbf4oHopkRqnPj+oaEMDHHjH6sO8bdvD1JRXdsm79VZ9HbvzfODn2fjzI38c8A/qZW1vLzzZUavGM2Tm59ka/pWavQ1xg5TURpocsxBCGEOvA+MBdKB3UKINVLKo5c0mwsUSClDhRCzgdeAWUKICGA20BvwBX4RQvSoO+ZKfd4F+APhUkq9EMKzNT5oR7E1Yyu1spbRAde+dkN1rZ5lSecYGupOD6+2u7/tYm/Forviefvnk7ynPc0BXRH/u61vm75nZ+Bo5cjs8NnM6jmLY/nHWHNmDWvPruWn1J/wsPVgSvAUpoVMI9Ql1NihKkqzrhz6A6ellGellFXAcmD679pMB5bUvf4GGCMM5T+nA8ullJVSyhTgdF1/V+vzAeBFKaUeQEqZ3fKP1/FodVo8bT2JcIu45mN/PHyBC8UV3DM0sPUD+x1zM8GT43uy+O548kormfq/bXy245y6zdQMQggi3CL4e/+/kzAzgXdGvkOkeyTLji5jxpoZ3PrDrXxx7As1JVYxquYkBw1w6ZqK6XXbGm0jpawBigC3qxx7tT5DMFx17BFC/CiECGssKCHEfXVt9uTkdI56N5W1lWzL2NbiQnsLt6UQ5G7PyB7td7E1sqcn6/4yjP5Brvxr9WHuX7aXglL1MFhzWZpbMqb7GP47+r/8MvMX/hr/V6r11fxn138Y8/UY7ttwH6tOrVKF/5R2Z4oD0tZAhZQyDvgEWNhYIynlfCllnJQyzsPDo10DbCs7z++kvKa8ReMN+9IKSNYVctfgQMzMRNMHtCJPRxuW3N2ff07qhfZENhPe3ULC8ax2jaEzcLN1Y07EHL6Z9g0rp61kbuRcdBd1PLv9WUZ+NZJHEh5hfcp6tdaE0i6a85xDBoYxgF/51W1rrE26EMICcALymjj2StvTgZV1r1cBi5oRY6eg1Wmxs7BrUaG9RYmpOFpbcHM/vzaIrGlmZoJ7hwczKMSNJ1Yc4J7Fe7ilnx/PTInAydbSKDF1ZGEuYYS5hPFw34c5nHuYH1N/5KeUn+qLMY7yH8X4wPEM9h2MjUXTRRUV5Vo1JznsBsKEEEEYTuCzgdt+12YNcCeQBNwCJEgppRBiDfCFEOItDAPSYcAuQFylz9XAKCAFGAGcbPnH6zh+LbQ3RDMEK/Nrm4J6vqicHw+d567BgThYG/e5xkiNE2seHsL/Np7mw81n2Hoqh1dvimZUeJeaV9BqhBBEeUQR5RHFE/2eYF/2Pn5M+ZEN5zawLmUdtha2DNUMZWz3sQzTDMPBysHYISudRJNnEilljRDiIeAnwBxYKKU8IoR4EdgjpVwDLACWCSFOA/kYTvbUtVsBHAVqgD9LKWsBGuuz7i1fBT4XQjwGlADzWu/jmq4juUfILc9t0S2lZUnn0EvJnYMDWz+wFrC2MOfJ8T0Z19uLp74+yN2LdzOjr4Z/Tu6lnqy+DuZm5sR7xxPvHc/TA55mz4U9bEzbyMa0jfx87hDEX4MAACAASURBVGcszSwZ5DuIGwJuYKT/SFxsrq3goqJcSnSG2SVxcXFyz549xg7juvx3339ZeHghm2dtvqZ6SuVVtQx+dSP9g1z5eE5cG0bYMpU1tbyfYLiKsLU0528Tw7k1PqDdx0U6M73UcyDnAL+c+4WNaRvJKMnATJgR5xXH6IDRjPAbgZ+jcW43KqZNCLG3bnz38n0qOZiGGd/NwMXGhYXjGx1/v6Ivd6Xx9MpDLL9vIAOD3doouut3OruEf60+xI6z+cT4O/PyjZFEalpWVFC5Miklx/OP8/O5n/kl7RdSilIACHUOZbjfcEb4jSDaIxoLM1VWTVHJweTpinVMWjWJv8b/lTkRc5p9nJSS8e9swcLMjLWPDMXwaInpklKyOjmDf689Rn5pFXcMCuQvY8JavcyH8ptzxefYkr6Fzemb2XthLzWyBidrJ4ZqhjLCbwSDfQe3uPKv0vFdLTmoPx9MQIIuAeCaxxsST+dxMquEN26JNvnEAIbB1Rl9/Rjd04s3NhxnaVIqq/Zn8MiYMOYM7I6VhSnOrO7YunfrzpyIOcyJmMPFqoskZSaxOX0zW9O3svbsWsyFOTGeMQzTDGOIZgg9XHq06BkbpfNRVw4m4K71d1FcVczKaSubbnyJuYt3cyC9kG1/G42NpXkbRdd2jl8o5t9rj7H1VC5B7vY8PTGcsRFeHSLRdXS1+loO5x1ms24zW9K3cKLgBACuNq4M8h3EEN8hDPIdhLutu5EjVdqSuq1kwgorChmxYgTzoubxcN+Hm31cSm4po/5vE4+MCePxsT2aPsBESSnZdCKHl9ce5UxOKYOC3fjbxHBi/J2NHVqXklOWQ9L5JLZnbicpM4n8CsPa4T1cetSvVRHrFdvi9cwV06RuK5mwLRlb0Es9o/2vrdDeku2pWJoL/jiwddZsMBYhBKPCPRka5s6Xu9J455dT3Ph+Ijf08uKJcT3o5dPN2CF2CR52HkwLmca0kGnopZ4T+SfqE8Xnxz5n8ZHFWJtb08+rH/29+9Pfuz+93Hqpge1OTF05GNlj2sc4mHOQn2f+3Ox7vcUV1Qx6ZSPje3vz1qyYNo6wfZVU1rBoWwrzt56lpLKGKdG+PHZDGMEe6uEuYymrLmNP1h6SMpNIykziTNEZABwsHYj1iqW/d3/ivePp6dITc7OOd3uzK1NXDiaqsraSxMxEpgZPvaZBwBW7dZRW1bbZmg3G5GBtwcNjwpgzqDvzt5xlUWIq6w6d56a+Gh4cFUqQu72xQ+xy7CztGO43nOF+wwHILc9lz4U97Lqwi90XdrMlfQtgKEke5xVXnyzCXMLU4HYHppKDEdUX2ruG5UBr9ZLF21OJD3Qhyq/zTkF0trPirxPCuXtIEB9sOs0XO9P4dl86k6J8+POoUHW7yYjcbd2ZEDSBCUETAMgqzWJ31m52X9jNrvO70OoMy9w6WzvT17MvsZ6x9PXqS4RrBJbmqs5WR6GSgxElpCVgb2lPf+/+zT7ml2NZpBeU889JvdowMtPh4WjNc1N788DIEBZsS+GzpHP8cPA8Y8I9+fPoUGIDVIkIY/Oy92JK8BSmBE8B4HzJ+fqriv3Z++uThbW5NVHuUYaE4RVLH48+OFqpBaJMlRpzMBK91DPm6zHEesby5sg3m33crI+TSC8oZ/NTI7Ew73qX7EVl1SxJSmVhYgqFZdUMDHZl3tBgRod7qpIcJiq3PJf92fvZl7WP5OxkjuUfo1bWIhD0cOlRnyz6evbF297b2OF2KWrMwQQdzj1sKLR3DbeUjmQWsTMln39MCu+SiQHAyc6SR8aEMXdoEF/sTGNhYgrzlu4h0M2Ou4cEcUs/P+yNXJlWacjd1p2x3ccytvtYwDDAfSj3EPuy97E/az9rzqxh+YnlAHjaehLlEUW0RzRR7lH0duuNnaWdMcPvstS/IiPR6rSYC3OGaYY1+5hFianYWpozK65jT19tDfbWFtw7PJi7hgSy/vAFFmxL4bk1R3hzwwlu7R/AnYMD8XW2NXaYSiPsLA1rlvy6bkmNvoaTBSfZn72fQ7mHOJRziI1pGwEwE2aEOocS7RFNtLshYQQ7B6uB7nagbisZyY2rb8TN1o0F4xc0q31uSSWD/5PArHh/Xroxso2j65j2pRWwYFsK6w9fAGBsLy9uGxDA0FB3dcupgymoKDAkitxDHMw5yKHcQ1ysuggYptD2du9NtHs0ke6RRLhF4GWnnqxvCXVbycSkFadxpugMt/S4pdnHfL4jjapaPXcNCWy7wDq42AAXYm9zIaOwnGVJ5/h6j44NRzJ5yHEL/bt3o9e4ubh5+ho7TKUZXGxcGkyf1Us954rP1SeKgzkHWXh4IbWG5WFwtXElwi2i/qu3W2+VMK6TSg5G8OvsjeaON1TW1PLZznOM7OlBiHoYrEkaZ1v+PjGcx4Z5UbjsDryytsBpqDr1Nvsch2I94G4ihkxDqAe2OgwzYUaQUxBBTkFMD50OQHlNOSfyT3A076jhK/8oSZlJDRJGL9deDZKGj72PShjNpJKDEWh1Wnq49EDjoGlW+7UHz5NzsbJTPvTWZnJPYf3lrXgVpMDkN0lzjCEz4RN6Zq/FZeNmLiR4kOI3g4Ax96IJ7Li1qboyWwtbYjxjiPH8rUpAeU05JwtO/pYw8o42uMJwsXahl1sverr2pKeL4SvQKVCVAWmE+om0s4KKAvZn7+feqHub1V5KycLEFEI9HRgepipkNsvJDfDtXDC3gjvWQOAQAoCA8DgqysvYufFLbA99zoC0T2DRJxy0jqG8181EjL4dRydXY0evXAdbC1v6ePShj0ef+m0VNRWcKjhVf3VxNO8onx39jGp9NQBWZlaEOIfQw6XHb0nDtWeXX+dCJYd2tiXdUGivubeU9pwr4HBGMS/fGKkuh5siJWx7Cza+BN5RMPsLcPZv0MTG1o4BU+bClLlcSDtJ2sZP8Etbg++Bf1GR/AL7ug3FMmYW4cNmYGllY6QPorQmGwsbojyiiPKIqt9Wra8mpSiFE/knOFlwkhP5J9iasZXvznxX38bLzqs+WfRw7UEPlx50d+zeZepHqdlK7exR7aMcyj3EL7f80qyT/YOf7yXxdB5JT4/Gzkrl8iuqKoXvHoIjKyHyZpj2Hlg1b3681Os5sU9LYdJn9Mj7GVcuUogDx11vwD7uViIGjMXcvGucELq63PJcTuaf5ETBCcNX/glSi1KpkTWA4SnvYKdgQpxDCHEOIdQ5lBDnEDQOmg45vVbNVjIRFTUVbM/czrSQac1KDOkFZaw/fIF7hwerxHA1hWmw/Da4cBhueB6GPArXcJUlzMwIjxsDcWOoqKhg37bv0B/4ij5567DdsJqsDa6keIzBqd/N9Iwbi5mF+n/RWbnbuuOucWewZnD9tqraKs4UnuFEwQlOFZziTOEZdl/YzQ9nf6hvY2thS5BTUH2y+PW/PvY+HTJpgEoO7aq+0F4zlwNdlnQOIQR3DAps28A6stRtsOIOqK2B21ZAj3HX1Z2NjQ2xN8yCG2ZRXlJE8uavkEe/IyZ7NTbrvyZvvTNn3UdhG3MT4QMmYGGp1r/u7KzMrejl1otebg3rmV2susiZwjOcKTzD6cLTnCk8Q1JmEmvOrKlvY2dh1+AqI8gpiKBuQfg6+Jr87Sl1W6kdPb/9edanrmfLrC1YmV/9pFJWVcPAVzYyLMyD92+PbacIOxApYfensP7v4BoMs78E99A2e7uSi4Uc2/INHF1D75Id2IlK8nHkpPNwLCOm0mvIFOzsVRE5BYoqixokjF9f51Xk1bexNLOke7fuBDkFEdgtsH6abmC3QBys2m+6urqtZAL0Us8m3SaGaoY2mRgAvt2XQXFFDfcMDWz74DqamkpY9yTsWwph4+HmT8CmbWeWODg6Ez95HkyeR1lpMfu2rUYe+Y7IwgQctq+lPNGKZPt+1ISMJ3jIzbh6qxInXZWTtROxXrHEejX8o66wopDU4lRSilIMX8UpnCo4RUJaQv1UWwAPW4/Lk4ZTYLvfolLJoZ0cyj1EXkVes24p6fWSRYkpRPs5qZLUv3cxC1bMAd1OGPYEjPontPPluZ19N2LH3wHj76C6spwjO9dTcugHAnI243MoCQ49zymLMHJ9R+Hadxph0UMw66KFEpXfONs4E2PT8LkMgOraanQluvqkkVqUSkpxCj+m/lhfMgTAxtyGgG4BdO/Wvf4rsFsg4a7h2Fi0/sw6lRzaiTbNUGhvqGZok223nMrhbE4p78yKUdNXL5WxF5b/ESoKYeZi6D3D2BFhaW1L7+EzYPgMpF7PycN7yNm7GreMBAac+wSztPlkf+dKitMAzMPGEDpgCs4ePsYOWzEhluaWBDsFE+wU3GC7lJL8inxDwqi74kgtTuVUwSm0adr6GVSrpq0i1KX1b6mq5NBOtDotcV5xzXqwZmFiKp6O1kyKUieReslfwvd/AQcvmLvB8ByDiRFmZvSI7k+PaMPiTQXZGaQkrUKc/pnwoq047fkR/e6nOGUZSoH3ULpFjickdpR6nkJplBACN1s33GzdiPNuOCxQra8msySTc8XnCOjWNrcwVXJoB+eKz3G26Cx/6PmHJtuezr7IlpM5PDG2B1YW6lYEtTXw87Ow430IHAYzl4C9m7GjahYXTw0u0x8CHqK2poYTyVvIO7ge58ytxOqWYJG+iNIfbThsF0OZ/0i8+owjOLyvugWlNOnXAe3u3bq32Xuo5NAOtGl1hfaaMd6wKDEVKwszbhugBjQpy4dv7oazm6D//TD+39BB1yA2t7CgZ9xoiBsNQGFBLmd3raP65Eb885PwPbkDTr5KLs6cc+yLPmAomr7j8A2JuqZnNhSltajk0A60Oi09XXri63D1ctGFZVWs3JfBjTG+uDlYt1N0JirrKCy/FYozDU87x84xdkStytnFvX5QGyD73DHS9v6ESN2Gf/FePI9o4chL5OJMmmMs1QFD8e5zAwGhUQgzdWWhtD2VHNpYQUUByTnJ3Bd9X5Ntl+/WUV5dq6qvHvseVt4P1g5w11rw72/siNqcZ/deeHbvBTyK1Os5d/owmck/Y6FLJPDiPjyOJMCRF8nGlTT7aKp94/GIGE5g5ED1IJ7SJpqVHIQQE4B3AXPgUynlq7/bbw0sBfoBecAsKWVq3b6ngblALfCIlPKnZvb5X+AeKWWHXsBgc/pm9FLPSP+RV21XU6tn6fZUBgW70cunW/sEZ2r0etj8Gmx+FTT9YNZn0K3rLc4jzMzo3iOa7j2igSeQej3pZw5z4eAvmKUl4ld8AO9Tm+DUG5SttuakTTglnnHYhwwmIGYkjs6qeq9y/ZpMDkIIc+B9YCyQDuwWQqyRUh69pNlcoEBKGSqEmA28BswSQkQAs4HegC/wixDi1+L5V+xTCBEHdIoJ/to0LV52XkS4Rly13U9HssgsquD5ab3bKTITU3kRVv0Jjv8AfW6DKW+DpZrFA4Zk4RcWjV9YNPA4ANnpZ0k7kEB1ShLu+fuJTVuEhW4Beq0gxTyALKcYREB/PHsNpXtYFGaqcKByjZpz5dAfOC2lPAsghFgOTAcuTQ7TgefrXn8DvCcME/SnA8ullJVAihDidF1/XKnPumT0BnAbYPyJ7NehoqaCpPNJzSq0tygxhQBXO8b08mqn6ExI/ln48jbIPQnj/wMDH1CDsE3w9AvG0y8YmAdAUVEBaQe3UHoqEfvsPUQWbMCh4Ds4AMXSjjSbnpS698E2KB7/qOG4eKkJD8rVNSc5aADdJd+nAwOu1EZKWSOEKALc6rbv+N2xvy5/dqU+HwLWSCnPX+2EKoS4D7gPICDANH/Rd5zfQXlNOaP9R1+13cH0QvacK+CZKRGYm3Wxk+KZBPj6bkMymLMSgkcaO6IOycnJhahh02GYYQlNWVuD7tR+so4loU/fg2vhYXqmL8MyYzFsg2zcyLQPp9wzBrugAQREDsHFVd2OUn5jUgPSQghfYCYwsqm2Usr5wHwwFN5r28haZpNuEw6WDsR7x1+13aLEVBysLfhDnF87RWYCpISk9+HnZ8Aj3LAwj2sXH4hvRcLcAv/wePzDf/vdKyu9yMnDOyg8tQPLrGR8So7in5IIKe9DAqQJH7LtelLtGYl991j8Iwbi4tm8pWyVzqc5ySEDuHQ5Lb+6bY21SRdCWABOGAamr3ZsY9v7AqHA6bqrBjshxGkpZduV22wjlxbas7zK3Pzs4gp+OJjJ7QO642jTMefwX7Pqcvj+UTi4HHpNhRs/MsxMUtqUnb0jvQeMhQFj67cVFeSQfjiR0rM7sco5hG/pMXxTNkEKsAlycOW8XRjlbr2x9ovBs0d/fLr3VNNpu4DmJIfdQJgQIgjDCXw2hvGAS60B7gSSgFuABCmlFEKsAb4QQryFYUA6DNgFiMb6lFIeAbx/7VQIUdIREwPAwZyD5FXkNTlL6bMd56jRS+4aHNgucRldUQZ8dTtk7jcUzRv2JKgTjdE4uXjgNOxGGHZj/baighx0R3dQfHYfFtmH8Cg9QUTabix0ekgyjGHorEIodO6FmVdvXIL64N8jFnvHrr3mcmfTZHKoG0N4CPgJw7TThVLKI0KIF4E9Uso1wAJgWd2Acz6Gkz117VZgGLyuAf4spaE2bWN9tv7HMx6tTouFsGCY37ArtqmoruXznWmMCfck0N2+HaMzkrSd8NUfobrMcBspfLKxI1Ia4eTigdOQqTBkav22stKLnDm+h6KUvYjzB3EuPk5s9mpsc1bAYdBLQYaZFzl2IVS6hmOticQ9OBaf4N6YW3SRK+JORi3200amrZ6Gp50nn4779IptVuzR8ddvDvL5vAEMCe3kg4F7F8PaJ8HZ35AYPHs1eYhi2mRtDRfOHSfr9H4q0g9hmX8c99Iz+OkzMBeG80qVtEBn4U+BfQhVbuFY+UbjGRyNb/cwLNRyq0anFvtpZ6lFhvK6s3rOumIbKSULt6XQ08uRwSEdo5Bci9RWw/qnYfcnEDIablkItp3iEZYuT5hb4BMciU9wZIPtpaUlpJ86QGFqMvqso9gVnMCvOBnv4l8MYxmJUC6tSLXwo9AuiBqXUKx9e+EeGIlPUCQW1rbG+UBKAyo5tIFNuk3A1Qvt7Tibz/ELF3n1pqjOu2ZDaS6suBPObYPBD8OY58Fc/cp1dvb2DvSMGQIxQxpsLynK48LJfRTqjlCbfRzbojP4lhzGuzgBszQJO6BWCtLNvMmz7U55txDMPHvioOmNV0gUrm6enfffiglS/1LbgFanJdw1/KqF9hYmpuBiZ8mNfTvpVMHzB2H5bVCaAzd9AtFNlytXOjcHJzdC48dC/NgG28tKi0k/fZiCc4epzjqOTeEZXMtTCC/di/WFajhoaJeHE1kWGi7aB6B3DsbKqwfOfuH4Bkdg66AGw1ubSg6tLL8in+ScZO6Pvv+KbdLyyvjlWBYPjgzBxrITljU4/C2s/jPYucI968G3r7EjUkyYnX03evQZDH0GN9heW1NDZtpJclMPUXH+GGZ5p7ErOUdw0S48itbDOQxzH4FcnMm18uOifXdqXYKx8gylmyYc78BeOKhZVC2ikkMr26xrutDe4u2pmAvBnIGB7RZXu9DXQsJLsO1t8B8Is5aBg6exo1I6KHMLC3yDI/ANvrwuWVlJEZkpRynUHacq+xTmBWdxKE0jsCARj4K1cPa3ttm4km2pocQugFqnACzdA+nmE4a7fw/cPDXqmY0rUMmhlWl1Wrztvenl2vhsnIsV1azYo2NSlA/eTp2osFxFEXw7D05tgH53wcQ3wEKVklbahp2DE6FRgyBq0GX7igrzyU49RnHmcaqzT2NReBbH0jRCixJxL1oLab+1LZPWZJt7UWSjocrRH5y7Y+0ZgpNPCB4BPbBzcG7HT2VaVHJoReU15SRlJnFj6I1XHDj7Zm86JZU13DO0E5WKyDlpWJinIBUmvwXxc40dkdKFOTm74tTIgDhARdlFstJOUZh5kvKss8iCVKwv6nCqyMSrdD8OWRVw4rf2+XQjx8KHYhtfqhz9ES6B2Lp3x9knGA+/kE59y0olh1a0I3MHFbUVjApofJaSXi9ZvD2VvgHOxPh3kr9ITv5kuGIwt4I71kDg5f8gFcVU2Ng50j08lu7hsZft09fqyck9T67uFCUXTlOTl4J50TnsyjLQlB7D4+IWLM/XNjimAEfyzD0psfamysEXnPyxdu+Og2cgLj7BuHhqEGYdc1xRJYdWtCm9rtCeV+OF9hKOZ3Mur4wnx/Vs58jagJSw9U1IeBm8owwPtjn7N32copgoM3MzPLw0eHhpaKz2p6ytJu+CjryM05Rkp1KVl4Yo1mFTlolTRToepftwyC6HU78dUyUtyDFzp8DSizJbH2ocNQhnf2zcuuPo2R1330CcnF1NcoquSg6tpFZfyybdJoZphl2x0N6i7Sl4d7NhQqR3o/s7jKpS+O7PcGQVRN5sWOPZys7YUSlKmxLmlrhpgnHTBDe6X+r15OfnkpNxhpKsVKryU6EoHcuSTBwqzhNYtBv3wp8wS29YlaJE2pJn5kaxlQcVtl7UOvhg5uSHjZuGbp6BuHp1x9HNq92vQFRyaCWHcg+RX5F/xVlKJy5cJPF0Hn+d0BNL8w48O6LgHCy/HbIOww0vwJC/qIV5FAXDin2u7p64unsClw+UA9RWV5FzPpWC82coy9VRWZABxZlYll3AviILz8LduBUUYJGub3BclbQgz8yVQgsPymw8qbb3AUcfLF38CRs8jW7OrV9lQSWHVpKgS8BCWDDUb2ij+xclpmBjacat8aa5MFGzpGyFr++E2hq4/WsIG9v0MYqi1DO3tMIjoAceAT2u2Ka6uprzWekUXkihJEdHdUE6svg8lqXnsavMxqvkOG7FidheqAIgLbSvSg6mTJumJc47jm5W3S7bl19axar9GdwU64eLfQec3ikl7PoE1v8d3EJg9pfg3iErqSuKybO0tMTHLwgfvyvPaJR6PYX5OeRfSMWve3ibxKGSQytIKUohtTiVW8NvbXT/l7vSqKzRc/eQwPYNrDXUVMLaJ2D/MugxAW6aDzadd/qeonQEwswMZ3cvnN3bbs15lRxawdUK7VXX6lmalMqwMHd6eDm2b2DX6+IF+GoOpO8yLMoz6p9qYR5F6SJUcmgFWp2WXq698HHwuWzfukPnySqu5D83RRkhsuuQvtewYltFEcxcDL1nGDsiRVHakfoz8DrlleeRnJ18xVlKixJTCXK3Z2SPDlRjKPlLWDQRzC1h7gaVGBSlC1LJ4TptSd+CRDZ6S2lfWgHJukLuGhyImVkHmO5ZWwPr/wGr/wT+/eHeTYYH3BRF6XLUbaXrlKBLwMfeh3DXy2cMLEpMxdHGglv6+RkhsmtUlg/f3A1nN0H/+2H8vw1XDoqidEkqOVyH8ppydmTuYEbYjMsefz9fVM66Q+e5e3Ag9tYm/mPOOmoonFecCdPfh75/NHZEiqIYmYmftUxbfaG9Rm4pLUs6h5SSOwcHtn9g1+LoGlj1J7B2gLvWgX/jdaEURelaVHK4DlqdFkdLR+K84xpsL6+q5YtdaYyN8MLf1URrDun1sPk12PwqaPrBrM+g25WXNVUUpWtRyaGFavW1bE7fzFDNUCzNGt6bX52cQWFZNfcMMdE1GyovGq4Wjv8AfW6DKW+DZSdaeEhRlOumkkMLHcw9SH5F/mVrN0gpWZSYQoRPN/oHuRopuqvIO2MonJd7Eia8CgP+pArnKYpyGZUcWkibpsXCzIKhmoaF9hJP53Eyq4T/m9nH9Gq0n95omJEkzGDOSggeaeyIFEUxUeo5hxbS6rTEe8XjaNWwJMbCxBTcHayY2ufyp6WNRkrY/j/4/BbopoF7tSoxKIpyVSo5tMCvhfZ+f0spJbeUhOPZ3D6gO9YWJrI0YHU5rLofNvwLwqfA3J/B1UTHQhRFMRnqtlILaHVa4PJCe4sTU7A0F9w+0ETWbCjKMNRHytxvKJo37ElVOE9RlGZRyaEFtGmGQnve9r8t91lUXs3Xe9OZ2scXT0cTmPmTtsNQUbW6zLC+c/hkY0ekKEoHov6MvEa55bkcyDlw2VXD13t0lFXVmsb01b2LYfEUw4Nt835RiUFRlGumrhyuUX2hvUvGG2r1ksXbU+kf6EqkxogL4dRWG1Zr2/0phIyGWxaCrYvx4lEUpcNq1pWDEGKCEOKEEOK0EOLvjey3FkJ8Vbd/pxAi8JJ9T9dtPyGEGN9Un0KIz+u2HxZCLBRCmFT1N22aFl97X3q69Kzf9vPRLNILyo270ltJDiydbkgMgx+B279RiUFRlBZrMjkIIcyB94GJQARwqxAi4nfN5gIFUspQ4G3gtbpjI4DZQG9gAvCBEMK8iT4/B8KBKMAWmHddn7AVlVWXkXQ+iZH+Ixs8w7AoMQWNsy1jI9puyb6rOn8APhkFGXvhpk9g3EtgZiKzpRRF6ZCac+XQHzgtpTwrpawClgPTf9dmOrCk7vU3wBhhOHtOB5ZLKSullCnA6br+rtinlHKdrAPsAkym3vWO8zuorK1scEvpSGYRO1PyuXNwdyzMjTCEc+gbWDAepB7uWQ/Rf2j/GBRF6XSaczbTALpLvk+v29ZoGyllDVAEuF3l2Cb7rLudNAdY31hQQoj7hBB7hBB7cnJymvExrt+vhfb6efWr37YoMRU7K3NmxbXz9FV9LfzyPHw7F3z6wH2bwLdv+8agKEqnZcqzlT4Atkgptza2U0o5X0oZJ6WM8/DwaPNgavW1bEnfwlC/3wrt5VysZE1yJjfH+uFk145DI+WF8MUs2PY29Lsb7vweHDrQMqSKopi85sxWygD8L/ner25bY23ShRAWgBOQ18SxV+xTCPEc4AHc34z42sWBnAPkV+Qz2n90/bYvdqZRVavnrvYciM45aViYpyAVJr8F8XPb770VRekymnPlsBsIE0IECSGsMAwwr/ldmzXAnXWvbwES6sYM1gCz62YzBcH/lUvZBQAADlFJREFUt3fv0VVVdwLHvz8eASRBICDPhJdQjFoRMyAqFKQoYCvWgoVq5eGj7egabceuwbHL5bB0rcG20652nFpb8FVqEMQWxzcSwEEeBkyB0CKREAIGCBAe8ggk+c0fZ6ee3Jubx8259+bx+6yVlZN9z9n7d/a5Ofvus+85m6F44wgR8xSRe4GbgVmqWtm43QtOdlH1B+2VlVfw8sZCxn+lJ0N6JscniF3vwB8mej2Hu1daw2CMiZk6ew6qWi4iDwLvAm2BxaqaJyILgBxVXQksAl4WkXzgGN7JHrfeq8BOoBx4QFUrAGrK0xX5LFAIbHDfCFqhqgsC2+MoqCrZRdmM6j2K5CSvIXhzWzFHviiLz01vqvDhL2D1k9Dnq/CdJdA1re7tjDEmSvW6CU5V3wLeCkl73Ld8DpgRYdungKfqk6dLb3I35hWcLKDwZCF3XebNrayqLF5fwKWXJDN2aI/YFn7+NPzlAch7Ha6YDrf+BpKa6OxyxpgWo8mdiJui7H3eg/bGp40HIKewlB0HTvLUt66I7ZwNpYXexDyHdsCkBd7NbU1tjghjTItkjUM9ZBdVf9De4v8r4OJO7bn96hjeglHwIbx6t/eV1TuXwdBJsSvLGGNCNOWvsjYJR84eYVvJtn/c+La/9Azv5h1k1qh0OiXF4C5kVdj0O+9RGJ17wH2rrWEwxsSd9RzqsLZoLYr+4yusL20oRES4e8yA4AsrL4M3fwyf/BGGTYHbn4OOXYIvxxhj6mCNQx2yi7wH7Q3rNozTZeVkbd7H5Ct607drp2ALOnXQm39h/2YY9xMY/+82MY8xJmGscajFmQtn2Fi8kenDpiMirNi6n5PnypkX9E1v+7d4M7adOwEzXoDLvxVs/sYY00DWONRiQ/EG70F7aROorFSe/2gvV/W/mJHpAT4KO/dP8MbDkNIL7nkPel8ZXN7GGBMlu25Ri+x92aQkpTCy10jW7i5hT8lp5l4/KJivr1aUwzuPwp9/CGmj4L411jAYY5oM6zlEUPWgvbH9xtK+TXueX7+XS1I6MPXKPo3P/MwxWDYHCtbC6B/ATU9C2yY1p5ExppWzxiGC3JJcSstKmZA+gfzDp1j3aQn/OmkYSe0a2dk6lAevzIJTxTDtGbj6rmACNsaYANllpQiy97kH7fW9gefX7yWpXRu+O7qRczbsXAl/mOR9ZXXOW9YwGGOaLOs51KDqQXuje4+mvDyJ17bu57YRfUlN7hBdhpWVsPY/Ye1C6JcJ3/kjdAng8pQxxsSI9RxqUHCigH2n9jEhbQJZHxdx7kIlc6N9+mrZKVh6l9cwjLgT5rxpDYMxpsmznkMNVhetBuCGvuOY8fpOxgxO5bI+UdypfPQzyPouHNkNkxfC6O/bg/OMMc2CNQ41yC7KJiM1g9y98PmJc/zHtCsankn+Klg+D6QNfG8FDB4fcJTGGBM7dlkpxJGzR9hesp0JaRNYvL6A9O4XcePwBszPrAof/QaWzIAu/eG+bGsYjDHNjvUcQqwpWoOi9EvKZEvhIR7/RgZt29TzUtCFs/DGQ7BtKVx2K9z2W+gQpylEjTEmQNY4hFhTtIZ+yf1YlduG5A7tmJFZzzkbTuz3JuYpzoUJP4Vxj9j4gjGm2bLLSj5VD9ob3Wssb+04yIzM/qR0rMedy4Ub4Lnx3gD0zFfgaz+xhsEY06xZz8Fnw+feg/ZOlw6nvFKZc93Aujfa8gK8+Qh0TYPZ/wuXDI91mMYYE3PWOPisLlpNSvsUPsjtzMThPRiQ2jnyyuXn4Z35kLMIhkyE6YugU4BPazXGmASyxsEpryxn3f51DLgokw2nK2qfs+GLElg2GwrXw3X/Al9/AtrEYMpQY4xJEGscnNzDuRwvO06bo4MZ3juFMUNSa17x81xv4PnMEbj99/DVO+IbqDHGxIENSDvZRdm0k/YUHkhn7vUDa56zYftyWDwZUJj3jjUMxpgWy3oOfPmgvc46HOmUwrQR/aqvUFkBHyyA9b+C9DFwx0uQ3IAb44wxppmxxgHYc2IPRaeKOHfwGr4/Kp2O7X3jB2ePw2v3Qv77cM1cmPI0tEtKXLDGGBMH1jjgXVIC0NMZfG/MgC9fKNnlTcxzvBBu+S/4p3sSFKExxsSXNQ7Aqr0foOf6c0vGcHp16egl7nobXrsP2nWA2W/AgOsSG6QxxsRRqx+QLjlTQt6xHZw/eZk3Z4MqrPu512NIHQz3r7GGwRjT6rT6nsPqfWsAGJp8LSN6tYdlc2Dnn+HKGfDNX0PSRQmNzxhjEqHVNw4r/v4ulee786OrBsCim+BQHkxa4N3cZs9HMsa0UvW6rCQik0Vkl4jki8j8Gl7vICJL3eubRGSg77VHXfouEbm5rjxFZJDLI9/lGbOvBp25cIa/Hd9CWlkfJn44E44XwZ3L4fqHrGEwxrRqdTYOItIWeAaYAmQAs0QkI2S1e4BSVb0U+CWw0G2bAcwELgcmA/8jIm3ryHMh8EuXV6nLOyaW5a1CpZwnvliDdO4J92fD0K/HqjhjjGk26tNzGAXkq+oeVT0PZAHTQtaZBrzolpcDE8W7xXgakKWqZapaAOS7/GrM021zo8sDl+dt0e9eLVRZ//HP6VJRwVV9x8K9qyB1SEyKMsaY5qY+jUM/oMj3936XVuM6qloOnABSa9k2UnoqcNzlEaksAETkfhHJEZGckpKSeuxGWAakd0jja1xKp7uWQscuDc/DGGNaqGY7IK2qzwHPAWRmZmo0efz07iWBxmSMMS1FfXoOB4A039/9XVqN64hIO+Bi4Ggt20ZKPwp0dXlEKssYY0yM1adx+BgY6r5FlIQ3wLwyZJ2VwGy3PB1Yrarq0me6bzMNAoYCmyPl6bbJdnng8vxL9LtnjDEmGnVeVlLVchF5EHgXaAssVtU8EVkA5KjqSmAR8LKI5APH8E72uPVeBXYC5cADqloBUFOersh/A7JE5EngE5e3McaYOBLvw3rzlpmZqTk5OYkOwxhjmhUR2aKqmTW91uqfrWSMMSacNQ7GGGPCWONgjDEmjDUOxhhjwrSIAWkRKQEKo9y8B3AkwHCCYnE1jMXVMBZXw7TUuAaoas+aXmgRjUNjiEhOpNH6RLK4GsbiahiLq2FaY1x2WckYY0wYaxyMMcaEscbBPbyvCbK4GsbiahiLq2FaXVytfszBGGNMOOs5GGOMCWONgzHGmDCtunEQkckisktE8kVkfozLShORbBHZKSJ5IvKQS39CRA6ISK77merb5lEX2y4RuTlWcYvIXhHZ7srPcWndReR9Edntfndz6SIiv3ZlbxORkb58Zrv1d4vI7Ejl1TOmr/jqJFdETorIw4mqLxFZLCKHRWSHLy2wOhKRa9wxyHfbSiPi+pmI/N2V/bqIdHXpA0XkrK/unq2r/Ej7GGVcgR078R73v8mlLxXv0f/RxrXUF9NeEcmNZ31J5HNDYt9fqtoqf/AeFf4ZMBhIAv4KZMSwvD7ASLecAnwKZABPAI/UsH6Gi6kDMMjF2jYWcQN7gR4haU8D893yfGChW54KvA0IcC2wyaV3B/a4393ccrcAj9VBYECi6gsYB4wEdsSijvDmObnWbfM2MKURcd0EtHPLC31xDfSvF5JPjeVH2sco4wrs2AGvAjPd8rPAD6ONK+T1XwCPx7O+iHxuSOj7qzX3HEYB+aq6R1XPA1nAtFgVpqrFqrrVLZ8C/kaE+bGdaUCWqpapagGQ72KOV9zTgBfd8ovAbb70l9SzEW/mvj7AzcD7qnpMVUuB94HJAcUyEfhMVWu7Cz6m9aWq6/DmKgkts9F15F7roqob1ftPfsmXV4PjUtX39Mt52DfizagYUR3lR9rHBsdViwYdO/ep90ZgeZBxuXzvAF6pLY+g66uWc0NC31+tuXHoBxT5/t5P7SfrwIjIQOBqYJNLetB1Dxf7uqGR4otF3Aq8JyJbROR+l9ZLVYvd8kGgVwLiqjKT6v+wia6vKkHVUT+3HIsY5+F9UqwySEQ+EZG1IjLWF2+k8iPtY7SCOHapwHFfAxhUfY0FDqnqbl9aXOsr5NyQ0PdXa24cEkJEkoHXgIdV9STwW2AIMAIoxuvWxtsNqjoSmAI8ICLj/C+6TxsJ+c6zu5Z8K7DMJTWF+gqTyDqKREQew5uBcYlLKgbSVfVq4MfAn0SkS33zC2Afm+Sx85lF9Q8hca2vGs4NUecVhNbcOBwA0nx/93dpMSMi7fEO/hJVXQGgqodUtUJVK4Hf43Wla4sv8LhV9YD7fRh43cVwyHVHq7rRh+MdlzMF2Kqqh1yMCa8vn6Dq6ADVL/00OkYRmQN8A7jTnVhwl22OuuUteNfzh9VRfqR9bLAAj91RvEsp7ULSo+byuh1Y6os3bvVV07mhlrzi8/6qa1Cipf7gzZ+9B28ArGqw6/IYlid41/p+FZLex7f8I7xrrwCXU32Qbg/eAF2gcQOdgRTf8kd4YwU/o/pg2NNu+RaqD4Zt1i8HwwrwBsK6ueXuAdRbFjC3KdQXIQOUQdYR4QOGUxsR12S8edt7hqzXE2jrlgfjnSBqLT/SPkYZV2DHDq8n6R+Q/udo4/LV2dpE1BeRzw0JfX/F5ETYXH7wRv0/xftE8FiMy7oBr1u4Dch1P1OBl4HtLn1lyD/QYy62Xfi+XRBk3O5N/1f3k1eVH9513Q+A3cAq35tMgGdc2duBTF9e8/AGE/PxndAbEVtnvE+JF/vSElJfeJcbioELeNds7wmyjoBMYIfb5r9xTy+IMq58vGvPVe+zZ92633bHOBfYCnyzrvIj7WOUcQV27Nz7drPb12VAh2jjcukvAD8IWTcu9UXkc0NC31/2+AxjjDFhWvOYgzHGmAiscTDGGBPGGgdjjDFhrHEwxhgTxhoHY4wxYaxxMMYYE8YaB2OMMWH+HzGxIpWHijOBAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVWDJLXrUgRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LabelSmoothing(nn.Module):\n",
        "    \"Implement label smoothing.\"\n",
        "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False)\n",
        "        self.padding_idx = padding_idx\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.size = size\n",
        "        self.true_dist = None\n",
        "        \n",
        "    def forward(self, x, target):\n",
        "        assert x.size(1) == self.size\n",
        "        true_dist = x.data.clone()\n",
        "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        true_dist[:, self.padding_idx] = 0\n",
        "        mask = torch.nonzero(target.data == self.padding_idx)\n",
        "        if mask.dim() > 0:\n",
        "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "        self.true_dist = true_dist\n",
        "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gXwt5HVUgRT",
        "colab_type": "code",
        "outputId": "e58d6afb-8bf5-4a8c-8def-6b0abfa2083b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "#Example\n",
        "crit = LabelSmoothing(5, 0, 0.5)\n",
        "predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n",
        "                             [0, 0.2, 0.7, 0.1, 0], \n",
        "                             [0, 0.2, 0.7, 0.1, 0]])\n",
        "v = crit(Variable(predict.log()), \n",
        "         Variable(torch.LongTensor([2, 1, 0])))\n",
        "\n",
        "# Show the target distributions expected by the system.\n",
        "plt.imshow(crit.true_dist)\n",
        "None"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHBHTwmjUgRU",
        "colab_type": "code",
        "outputId": "f4e00472-5e7d-440a-c2c5-c7a04cd85c26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# Label smoothing starts to penalize the model \n",
        "# if it gets very confident about a given choice\n",
        "crit = LabelSmoothing(5, 0, 0.2)\n",
        "def loss(x):\n",
        "    d = x + 3 * 1\n",
        "    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d],\n",
        "                                 ])\n",
        "    #print(predict)\n",
        "    return crit(Variable(predict.log()),\n",
        "                 Variable(torch.LongTensor([1]))).data[0]\n",
        "plt.plot(np.arange(1, 100), [loss(x) for x in range(1, 100)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff46cbbd630>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVKyONFsUgRW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_backprop(generator, criterion, out, targets, normalize):\n",
        "    \"\"\"\n",
        "    Memory optmization. Compute each timestep separately and sum grads.\n",
        "    \"\"\"\n",
        "    assert out.size(1) == targets.size(1)\n",
        "    total = 0.0\n",
        "    out_grad = []\n",
        "    for i in range(out.size(1)):\n",
        "        out_column = Variable(out[:, i].data, requires_grad=True)\n",
        "        gen = generator(out_column)\n",
        "        loss = criterion(gen, targets[:, i]) / normalize\n",
        "        total += loss.data[0]\n",
        "        loss.backward()\n",
        "        out_grad.append(out_column.grad.data.clone())\n",
        "    out_grad = torch.stack(out_grad, dim=1)\n",
        "    out.backward(gradient=out_grad)\n",
        "    return total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiTlbMq2UgRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_std_mask(src, tgt, pad):\n",
        "    src_mask = (src != pad).unsqueeze(-2)\n",
        "    tgt_mask = (tgt != pad).unsqueeze(-2)\n",
        "    tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
        "    return src_mask, tgt_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSF9AaKJUgRZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch(train_iter, model, criterion, opt, transpose=False):\n",
        "    model.train()\n",
        "    for i, batch in enumerate(train_iter):\n",
        "        src, trg, src_mask, trg_mask = \\\n",
        "            batch.src, batch.trg, batch.src_mask, batch.trg_mask\n",
        "        out = model.forward(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
        "        loss = loss_backprop(model.generator, criterion, out, trg[:, 1:], batch.ntokens) \n",
        "                        \n",
        "        model_opt.step()\n",
        "        model_opt.optimizer.zero_grad()\n",
        "        if i % 10 == 1:\n",
        "            print(i, loss, model_opt._rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4kFYs7nUgRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def valid_epoch(valid_iter, model, criterion, transpose=False):\n",
        "    model.test()\n",
        "    total = 0\n",
        "    for batch in valid_iter:\n",
        "        src, trg, src_mask, trg_mask = \\\n",
        "            batch.src, batch.trg, batch.src_mask, batch.trg_mask\n",
        "        out = model.forward(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
        "        loss = loss_backprop(model.generator, criterion, out, trg[:, 1:], batch.ntokens) \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJo5AZasUgRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Batch:\n",
        "    def __init__(self, src, trg, src_mask, trg_mask, ntokens):\n",
        "        self.src = src\n",
        "        self.trg = trg\n",
        "        self.src_mask = src_mask\n",
        "        self.trg_mask = trg_mask\n",
        "        self.ntokens = ntokens\n",
        "    \n",
        "def data_gen(V, batch, nbatches):\n",
        "    for i in range(nbatches):\n",
        "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
        "        src = Variable(data, requires_grad=False)\n",
        "        tgt = Variable(data, requires_grad=False)\n",
        "        src_mask, tgt_mask = make_std_mask(src, tgt, 0)\n",
        "        yield Batch(src, tgt, src_mask, tgt_mask, (tgt[1:] != 0).data.sum())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrpU5b2sUgRe",
        "colab_type": "code",
        "outputId": "0fdffeaf-fb41-4274-e537-4f5a3996a9ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "V = 11\n",
        "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
        "model = make_model(V, V, N=2)\n",
        "model_opt = get_std_opt(model)\n",
        "for epoch in range(2):\n",
        "    train_epoch(data_gen(V, 30, 20), model, criterion, model_opt)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 2.8770084381103516 6.987712429686844e-07\n",
            "11 2.6362569332122803 4.192627457812107e-06\n",
            "1 2.5864903330802917 7.686483672655528e-06\n",
            "11 2.262630194425583 1.118033988749895e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-rkEm89l0Nas"
      },
      "source": [
        "# A Real World Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lPk71hKN_uxO",
        "colab": {}
      },
      "source": [
        "# For data loading.\n",
        "from torchtext import data, datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKynVliVXg6F",
        "colab_type": "code",
        "outputId": "2537b808-15d4-4056-c1fe-63de567408d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install torchtext spacy\n",
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.2.3)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.38.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (46.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.38.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.21.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Requirement already satisfied: de_core_news_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz#egg=de_core_news_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.21.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (46.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.6.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.38.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.18.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.8)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9ai2-MIARRC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')\n",
        "\n",
        "def tokenize_de(text):\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "BOS_WORD = '<s>'\n",
        "EOS_WORD = '</s>'\n",
        "BLANK_WORD = \"<blank>\"\n",
        "SRC = data.Field(tokenize=tokenize_de, pad_token=BLANK_WORD)\n",
        "TGT = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, \n",
        "                 eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
        "\n",
        "MAX_LEN = 100\n",
        "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(SRC, TGT), \n",
        "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
        "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
        "MIN_FREQ = 1\n",
        "SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n",
        "TGT.build_vocab(train.trg, min_freq=MIN_FREQ)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2nNeRS4AWZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Detail. Batching seems to matter quite a bit. \n",
        "# This is temporary code for dynamic batching based on number of tokens.\n",
        "# This code should all go away once things get merged in this library.\n",
        "\n",
        "BATCH_SIZE = 4096\n",
        "global max_src_in_batch, max_tgt_in_batch\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
        "    global max_src_in_batch, max_tgt_in_batch\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)\n",
        "\n",
        "class MyIterator(data.Iterator):\n",
        "    def create_batches(self):\n",
        "        if self.train:\n",
        "            def pool(d, random_shuffler):\n",
        "                for p in data.batch(d, self.batch_size * 100):\n",
        "                    p_batch = data.batch(\n",
        "                        sorted(p, key=self.sort_key),\n",
        "                        self.batch_size, self.batch_size_fn)\n",
        "                    for b in random_shuffler(list(p_batch)):\n",
        "                        yield b\n",
        "            self.batches = pool(self.data(), self.random_shuffler)\n",
        "            \n",
        "        else:\n",
        "            self.batches = []\n",
        "            for b in data.batch(self.data(), self.batch_size,\n",
        "                                          self.batch_size_fn):\n",
        "                self.batches.append(sorted(b, key=self.sort_key))\n",
        "\n",
        "def rebatch(pad_idx, batch):\n",
        "    \"Fix order in torchtext to match ours\"\n",
        "    src, trg = batch.src.transpose(0, 1), batch.trg.transpose(0, 1)\n",
        "    src_mask, trg_mask = make_std_mask(src, trg, pad_idx)\n",
        "    return Batch(src, trg, src_mask, trg_mask, (trg[1:] != pad_idx).data.sum())\n",
        "\n",
        "train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=0,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                        batch_size_fn=batch_size_fn, train=True)\n",
        "valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=0,\n",
        "                        repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n",
        "                        batch_size_fn=batch_size_fn, train=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMTpAq_bAave",
        "colab_type": "code",
        "outputId": "fdfcf546-2ea4-47d1-c2b2-9d6b0282dc62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Create the model an load it onto our GPU.\n",
        "pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
        "model = make_model(len(SRC.vocab), len(TGT.vocab), N=6)\n",
        "model_opt = get_std_opt(model)\n",
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EncoderDecoder(\n",
              "  (encoder): Encoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm(\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (src_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (src_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): DecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (src_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): DecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (src_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): DecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (src_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): DecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (src_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512)\n",
              "            (1): Linear(in_features=512, out_features=512)\n",
              "            (2): Linear(in_features=512, out_features=512)\n",
              "            (3): Linear(in_features=512, out_features=512)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048)\n",
              "          (w_2): Linear(in_features=2048, out_features=512)\n",
              "          (dropout): Dropout(p=0.1)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm(\n",
              "            )\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm(\n",
              "    )\n",
              "  )\n",
              "  (src_embed): Sequential(\n",
              "    (0): Embeddings(\n",
              "      (lut): Embedding(131874, 512)\n",
              "    )\n",
              "    (1): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "  )\n",
              "  (tgt_embed): Sequential(\n",
              "    (0): Embeddings(\n",
              "      (lut): Embedding(57960, 512)\n",
              "    )\n",
              "    (1): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "  )\n",
              "  (generator): Generator(\n",
              "    (proj): Linear(in_features=512, out_features=57960)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv4GWhYwAeGb",
        "colab_type": "code",
        "outputId": "6852dd4f-fc6c-4425-847e-50c7cb7238c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\n",
        "criterion.cuda()\n",
        "for epoch in range(15):\n",
        "    train_epoch((rebatch(pad_idx, b) for b in train_iter), model, criterion, model_opt)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 9.22804081439972 6.987712429686844e-07\n",
            "11 9.363911151885986 4.192627457812107e-06\n",
            "21 9.00422790646553 7.686483672655528e-06\n",
            "31 8.966406971216202 1.118033988749895e-05\n",
            "41 8.837086796760559 1.4674196102342371e-05\n",
            "51 8.474352180957794 1.8168052317185794e-05\n",
            "61 8.493759781122208 2.1661908532029216e-05\n",
            "71 7.828642785549164 2.515576474687264e-05\n",
            "81 8.144538700580597 2.8649620961716057e-05\n",
            "91 8.34702811203897 3.214347717655948e-05\n",
            "101 7.423434853553772 3.56373333914029e-05\n",
            "111 7.640395566821098 3.913118960624633e-05\n",
            "121 7.8300253436900675 4.262504582108975e-05\n",
            "131 7.395266108214855 4.611890203593317e-05\n",
            "141 6.384412348270416 4.961275825077659e-05\n",
            "151 5.313997700810432 5.310661446562001e-05\n",
            "161 6.548005968332291 5.660047068046343e-05\n",
            "171 6.410948306322098 6.0094326895306855e-05\n",
            "181 6.355024512158707 6.358818311015028e-05\n",
            "191 6.179466664791107 6.70820393249937e-05\n",
            "201 5.625807583332062 7.057589553983712e-05\n",
            "211 5.863505013287067 7.406975175468054e-05\n",
            "221 5.217307925224304 7.756360796952397e-05\n",
            "231 5.340813413262367 8.10574641843674e-05\n",
            "241 5.663193587213755 8.455132039921081e-05\n",
            "251 5.365973152220249 8.804517661405423e-05\n",
            "261 5.209586337208748 9.153903282889765e-05\n",
            "271 5.377171345055103 9.503288904374107e-05\n",
            "281 5.388091418892145 9.85267452585845e-05\n",
            "291 5.2190543208271265 0.00010202060147342792\n",
            "301 5.280199307948351 0.00010551445768827134\n",
            "311 4.859667733311653 0.00010900831390311476\n",
            "321 5.650727019470651 0.00011250217011795819\n",
            "331 4.648333963006735 0.0001159960263328016\n",
            "341 4.846539679914713 0.00011948988254764501\n",
            "351 5.344404591247439 0.00012298373876248845\n",
            "361 5.016580792143941 0.00012647759497733186\n",
            "371 5.149977366439998 0.0001299714511921753\n",
            "381 5.131182576529682 0.00013346530740701871\n",
            "391 4.906007267534733 0.00013695916362186213\n",
            "401 5.35014879447408 0.00014045301983670557\n",
            "411 4.43318273127079 0.00014394687605154898\n",
            "421 3.9032845832407475 0.00014744073226639242\n",
            "431 3.777287730947137 0.00015093458848123583\n",
            "441 4.921554883476347 0.00015442844469607924\n",
            "451 4.60955998999998 0.00015792230091092268\n",
            "461 4.67305507324636 0.0001614161571257661\n",
            "471 4.670414038933814 0.0001649100133406095\n",
            "481 4.764831267297268 0.00016840386955545295\n",
            "491 4.690684252884239 0.00017189772577029636\n",
            "501 4.9790772361448035 0.00017539158198513977\n",
            "511 3.895740158855915 0.0001788854381999832\n",
            "521 4.317631348967552 0.00018237929441482662\n",
            "531 4.0374781775753945 0.00018587315062967003\n",
            "541 4.442816694267094 0.00018936700684451347\n",
            "551 4.93605066098462 0.00019286086305935688\n",
            "561 3.913258006796241 0.0001963547192742003\n",
            "571 4.741000257898122 0.00019984857548904374\n",
            "581 5.104979486735829 0.00020334243170388715\n",
            "591 4.43747971765697 0.0002068362879187306\n",
            "601 4.845088332425803 0.000210330144133574\n",
            "611 4.90091169741936 0.0002138240003484174\n",
            "621 4.0697120716795325 0.00021731785656326085\n",
            "631 4.390976779515768 0.00022081171277810424\n",
            "641 3.972387991154392 0.00022430556899294768\n",
            "651 4.582428100053221 0.00022779942520779112\n",
            "661 4.078138270415366 0.0002312932814226345\n",
            "671 4.957489489977888 0.00023478713763747794\n",
            "681 4.940390691510402 0.00023828099385232138\n",
            "691 4.1957552041858435 0.00024177485006716476\n",
            "701 3.7108526648953557 0.00024526870628200823\n",
            "711 3.3686201644595712 0.00024876256249685164\n",
            "721 4.628134311875328 0.00025225641871169505\n",
            "731 3.997109721414745 0.00025575027492653847\n",
            "741 4.276489513926208 0.0002592441311413819\n",
            "751 3.606390040833503 0.00026273798735622535\n",
            "761 4.632051252236124 0.00026623184357106876\n",
            "771 4.31184046123235 0.00026972569978591217\n",
            "781 4.640527937561274 0.0002732195560007556\n",
            "791 2.658094963990152 0.000276713412215599\n",
            "801 4.42448287922889 0.0002802072684304424\n",
            "811 4.401796454563737 0.0002837011246452859\n",
            "821 3.908189784735441 0.0002871949808601293\n",
            "831 3.939930433407426 0.0002906888370749727\n",
            "841 4.242308129556477 0.0002941826932898161\n",
            "851 4.691913864204253 0.0002976765495046596\n",
            "861 4.366375337354839 0.00030117040571950293\n",
            "871 4.830338249215856 0.0003046642619343464\n",
            "881 4.475418315269053 0.0003081581181491898\n",
            "891 3.786354306153953 0.0003116519743640332\n",
            "901 4.472669267910533 0.00031514583057887664\n",
            "911 4.319670493714511 0.0003186396867937201\n",
            "921 4.40061495223199 0.00032213354300856346\n",
            "931 4.1712525109487615 0.00032562739922340693\n",
            "941 3.353776540607214 0.00032912125543825034\n",
            "951 4.478792773094028 0.00033261511165309375\n",
            "961 4.389358625747263 0.00033610896786793716\n",
            "971 4.655158417292114 0.00033960282408278063\n",
            "981 3.842689939774573 0.000343096680297624\n",
            "991 4.464792453683913 0.00034659053651246746\n",
            "1001 4.1550259296782315 0.0003500843927273108\n",
            "1011 4.531229938191245 0.00035357824894215433\n",
            "1021 3.1350144734606147 0.0003570721051569977\n",
            "1031 4.2130825108033605 0.0003605659613718411\n",
            "1041 4.40761236770777 0.00036405981758668457\n",
            "1051 3.7650114889256656 0.000367553673801528\n",
            "1061 3.384261306375265 0.00037104753001637145\n",
            "1071 4.287173196673393 0.0003745413862312148\n",
            "1081 4.2888694340363145 0.0003780352424460582\n",
            "1091 4.227257467340678 0.0003815290986609017\n",
            "1101 3.5663356073200703 0.0003850229548757451\n",
            "1111 3.443718218244612 0.0003885168110905885\n",
            "1 3.8983309627510607 0.0003892155823335572\n",
            "11 4.313543129479513 0.0003927094385484006\n",
            "21 4.236692764796317 0.00039620329476324406\n",
            "31 4.152533867396414 0.00039969715097808747\n",
            "41 4.28392969982815 0.00040319100719293083\n",
            "51 3.728499015327543 0.0004066848634077743\n",
            "61 4.307746298916754 0.0004101787196226177\n",
            "71 3.982990011369111 0.0004136725758374612\n",
            "81 4.031616076128557 0.0004171664320523046\n",
            "91 3.6681257281452417 0.000420660288267148\n",
            "101 4.328392759140115 0.00042415414448199147\n",
            "111 4.1297251540236175 0.0004276480006968348\n",
            "121 3.8612675048643723 0.00043114185691167824\n",
            "131 3.3234498887322843 0.0004346357131265217\n",
            "141 2.9316391709726304 0.0004381295693413651\n",
            "151 3.853710602968931 0.0004416234255562085\n",
            "161 3.657446308527142 0.000445117281771052\n",
            "171 4.315956906066276 0.00044861113798589535\n",
            "181 3.8166843445505947 0.00045210499420073876\n",
            "191 3.775005342438817 0.00045559885041558223\n",
            "201 3.6889250279054977 0.00045909270663042564\n",
            "211 3.9661942550956155 0.000462586562845269\n",
            "221 4.0317675591795705 0.0004660804190601125\n",
            "231 2.5750471502542496 0.0004695742752749559\n",
            "241 3.9363861238816753 0.0004730681314897993\n",
            "251 3.5953055381251033 0.00047656198770464276\n",
            "261 3.488678874447942 0.00048005584391948617\n",
            "271 4.203302938956767 0.00048354970013432953\n",
            "281 4.101230412779842 0.00048704355634917305\n",
            "291 3.505302316509187 0.0004905374125640165\n",
            "301 3.371255552450748 0.0004940312687788598\n",
            "311 3.605485462643628 0.0004975251249937033\n",
            "321 3.5399473074357957 0.0005010189812085467\n",
            "331 3.690164131578058 0.0005045128374233901\n",
            "341 3.9450457661878318 0.0005080066936382335\n",
            "351 3.2092829206958413 0.0005115005498530769\n",
            "361 4.1493603159487975 0.0005149944060679205\n",
            "371 4.456596437768894 0.0005184882622827638\n",
            "381 3.580994499847293 0.0005219821184976072\n",
            "391 3.764316374436021 0.0005254759747124507\n",
            "401 3.746422869618982 0.0005289698309272941\n",
            "411 2.652451364789158 0.0005324636871421375\n",
            "421 3.6829214121680707 0.0005359575433569809\n",
            "431 3.0677785123698413 0.0005394513995718243\n",
            "441 3.7282008964102715 0.0005429452557866678\n",
            "451 3.2069862517528236 0.0005464391120015112\n",
            "461 3.266633650287986 0.0005499329682163546\n",
            "471 3.6782764992676675 0.000553426824431198\n",
            "481 3.985882880690042 0.0005569206806460415\n",
            "491 3.531659039668739 0.0005604145368608848\n",
            "501 3.396840203553438 0.0005639083930757282\n",
            "511 3.8242671880871058 0.0005674022492905717\n",
            "521 3.5285675568156876 0.0005708961055054152\n",
            "531 3.633787609396677 0.0005743899617202586\n",
            "541 3.7535226930340286 0.000577883817935102\n",
            "551 3.2708886489272118 0.0005813776741499454\n",
            "561 2.485812731087208 0.0005848715303647888\n",
            "571 2.4059966886416078 0.0005883653865796322\n",
            "581 3.6433410312747583 0.0005918592427944756\n",
            "591 3.388878355617635 0.0005953530990093192\n",
            "601 3.155956140020862 0.0005988469552241626\n",
            "611 2.947724404744804 0.0006023408114390059\n",
            "621 3.6928354799747467 0.0006058346676538494\n",
            "631 4.266238223573964 0.0006093285238686928\n",
            "641 3.2433411069214344 0.0006128223800835362\n",
            "651 2.6578751150518656 0.0006163162362983796\n",
            "661 3.1363683314993978 0.000619810092513223\n",
            "671 3.2384229353629053 0.0006233039487280665\n",
            "681 3.5717479173326865 0.0006267978049429099\n",
            "691 2.7481900709681213 0.0006302916611577533\n",
            "701 3.152357578277588 0.0006337855173725967\n",
            "711 3.6279479932272807 0.0006372793735874402\n",
            "721 3.8254589694552124 0.0006407732298022836\n",
            "731 3.596652321401052 0.0006442670860171269\n",
            "741 3.5270806616172194 0.0006477609422319704\n",
            "751 3.744454919404234 0.0006512547984468139\n",
            "761 3.6361351436935365 0.0006547486546616573\n",
            "771 2.756956067867577 0.0006582425108765007\n",
            "781 4.009501164850917 0.0006617363670913441\n",
            "791 3.1930301231332123 0.0006652302233061875\n",
            "801 2.933791190967895 0.0006687240795210309\n",
            "811 2.910490698646754 0.0006722179357358743\n",
            "821 2.9913465892896056 0.0006757117919507177\n",
            "831 2.7824615500867367 0.0006792056481655613\n",
            "841 3.2506067617214285 0.0006826995043804046\n",
            "851 2.4274996304884553 0.000686193360595248\n",
            "861 2.838570057414472 0.0006896872168100915\n",
            "871 3.701764078112319 0.0006931810730249349\n",
            "881 3.848554913478438 0.0006966749292397783\n",
            "891 3.4526427384698763 0.0007001687854546216\n",
            "901 3.488480363972485 0.0007036626416694653\n",
            "911 3.974131463881349 0.0007071564978843087\n",
            "921 3.6016985251335427 0.000710650354099152\n",
            "931 2.748417721129954 0.0007141442103139954\n",
            "941 3.2431443587411195 0.0007176380665288388\n",
            "951 3.552851426124107 0.0007211319227436822\n",
            "961 3.945995692134602 0.0007246257789585257\n",
            "971 3.61233422163059 0.0007281196351733691\n",
            "981 3.0922402505530044 0.0007316134913882126\n",
            "991 2.132171700708568 0.000735107347603056\n",
            "1001 3.45746414270252 0.0007386012038178994\n",
            "1011 3.2397949513979256 0.0007420950600327429\n",
            "1021 2.6220455020666122 0.0007455889162475863\n",
            "1031 4.086820700009412 0.0007490827724624296\n",
            "1041 3.166667271638289 0.000752576628677273\n",
            "1051 3.2549868982750922 0.0007560704848921164\n",
            "1061 3.0760668418370187 0.0007595643411069599\n",
            "1071 2.401858759112656 0.0007630581973218034\n",
            "1081 3.2865313892252743 0.0007665520535366468\n",
            "1091 2.6227589338086545 0.0007700459097514902\n",
            "1101 3.1407183104893193 0.0007735397659663336\n",
            "1111 2.725114575587213 0.000777033622181177\n",
            "1 3.80845550520462 0.0007777323934241457\n",
            "11 2.2174228439107537 0.0007812262496389892\n",
            "21 3.173512311419472 0.0007847201058538326\n",
            "31 3.038104720879346 0.000788213962068676\n",
            "41 3.1513950274093077 0.0007917078182835193\n",
            "51 2.009920402429998 0.0007952016744983629\n",
            "61 2.7765948900487274 0.0007986955307132063\n",
            "71 2.2143871318548918 0.0008021893869280496\n",
            "81 2.3230191441252828 0.000805683243142893\n",
            "91 2.5591078381985426 0.0008091770993577365\n",
            "101 3.2009000024117995 0.0008126709555725799\n",
            "111 2.4213000365998596 0.0008161648117874234\n",
            "121 3.0830581886693835 0.0008196586680022668\n",
            "131 3.3466034341108752 0.0008231525242171102\n",
            "141 2.3932764008641243 0.0008266463804319536\n",
            "151 2.3901236969977617 0.000830140236646797\n",
            "161 2.670963984914124 0.0008336340928616403\n",
            "171 2.3154220250435174 0.000837127949076484\n",
            "181 3.1299555795267224 0.0008406218052913274\n",
            "191 3.1931777174104354 0.0008441156615061707\n",
            "201 2.0586755657568574 0.0008476095177210141\n",
            "211 3.0802493339870125 0.0008511033739358575\n",
            "221 2.742908854968846 0.0008545972301507009\n",
            "231 3.0150227585691027 0.0008580910863655444\n",
            "241 3.1883854767656885 0.0008615849425803879\n",
            "251 3.9723056437032938 0.0008650787987952313\n",
            "261 3.154051310819341 0.0008685726550100747\n",
            "271 2.9326753029599786 0.0008720665112249181\n",
            "281 2.548780526034534 0.0008755603674397616\n",
            "291 3.1163424318656325 0.000879054223654605\n",
            "301 3.0676717319292948 0.0008825480798694483\n",
            "311 2.9788330285809934 0.0008860419360842917\n",
            "321 3.2493747028142934 0.0008895357922991352\n",
            "331 2.756887035444379 0.0008930296485139786\n",
            "341 2.344908542698249 0.0008965235047288221\n",
            "351 3.2891006547606594 0.0009000173609436655\n",
            "361 3.3290338465594687 0.0009035112171585089\n",
            "371 3.229312543011474 0.0009070050733733523\n",
            "381 2.915426197476336 0.0009104989295881957\n",
            "391 2.1865285728126764 0.0009139927858030391\n",
            "401 1.2333901259116828 0.0009174866420178827\n",
            "411 2.890593276708387 0.0009209804982327261\n",
            "421 2.569426457863301 0.0009244743544475694\n",
            "431 2.652693082112819 0.0009279682106624128\n",
            "441 2.7095347091089934 0.0009314620668772562\n",
            "451 2.8402419824851677 0.0009349559230920996\n",
            "461 1.937577786622569 0.0009384497793069431\n",
            "471 3.396598350685963 0.0009419436355217866\n",
            "481 3.086843092009076 0.00094543749173663\n",
            "491 2.800925313960761 0.0009489313479514734\n",
            "501 3.550899401170227 0.0009524252041663168\n",
            "511 3.131350426134304 0.0009559190603811601\n",
            "521 3.1310435547929956 0.0009594129165960037\n",
            "531 2.5643370538018644 0.0009629067728108471\n",
            "541 3.078570921206847 0.0009664006290256904\n",
            "551 2.955561647831928 0.0009698944852405339\n",
            "561 2.6713626738637686 0.0009733883414553773\n",
            "571 2.984711581200827 0.0009768821976702208\n",
            "581 3.0040371504146606 0.0009803760538850642\n",
            "591 2.870752532966435 0.0009838699100999076\n",
            "601 3.5566324464743957 0.000987363766314751\n",
            "611 2.932413148577325 0.0009908576225295944\n",
            "621 2.759557524230331 0.0009943514787444378\n",
            "631 2.6608934830874205 0.0009978453349592813\n",
            "641 2.5000501023605466 0.0010013391911741247\n",
            "651 2.056229238398373 0.001004833047388968\n",
            "661 2.449834418017417 0.0010083269036038115\n",
            "671 3.55760422597632 0.001011820759818655\n",
            "681 3.568668602849357 0.0010153146160334983\n",
            "691 2.624748273170553 0.001018808472248342\n",
            "701 2.6581428004428744 0.0010223023284631854\n",
            "711 2.8772107043478172 0.0010257961846780288\n",
            "721 2.2558972756378353 0.001029290040892872\n",
            "731 2.8610597239458 0.0010327838971077154\n",
            "741 2.5421140673570335 0.0010362777533225588\n",
            "751 2.3651725962990895 0.0010397716095374024\n",
            "761 2.7333234185935 0.0010432654657522458\n",
            "771 1.9675575043074787 0.0010467593219670892\n",
            "781 3.4766063723473053 0.0010502531781819327\n",
            "791 2.11074760556221 0.001053747034396776\n",
            "801 2.818645672639832 0.0010572408906116195\n",
            "811 2.5747579112648964 0.001060734746826463\n",
            "821 2.543077446636744 0.0010642286030413063\n",
            "831 3.6307586169650676 0.0010677224592561497\n",
            "841 2.6619203181471676 0.0010712163154709931\n",
            "851 2.9983629511043546 0.0010747101716858365\n",
            "861 2.945340975886211 0.00107820402790068\n",
            "871 3.138857963611372 0.0010816978841155234\n",
            "881 3.129708889988251 0.0010851917403303668\n",
            "891 2.62316364236176 0.0010886855965452102\n",
            "901 2.232371203834191 0.0010921794527600536\n",
            "911 2.2327867303974926 0.001095673308974897\n",
            "921 2.792006753385067 0.0010991671651897406\n",
            "931 2.4268197109922767 0.001102661021404584\n",
            "941 2.978345069626812 0.0011061548776194275\n",
            "951 2.278895828407258 0.0011096487338342707\n",
            "961 2.6758228791877627 0.001113142590049114\n",
            "971 2.4649431947618723 0.0011166364462639575\n",
            "981 2.975804700359731 0.0011201303024788011\n",
            "991 1.371960106305778 0.0011236241586936445\n",
            "1001 2.9780429290840402 0.001127118014908488\n",
            "1011 3.6703556313877925 0.0011306118711233314\n",
            "1021 1.7095388555899262 0.0011341057273381748\n",
            "1031 2.726009818725288 0.0011375995835530182\n",
            "1041 2.2261498528532684 0.0011410934397678616\n",
            "1051 2.5863735019229352 0.001144587295982705\n",
            "1061 2.5925163053907454 0.0011480811521975484\n",
            "1071 2.4022681838832796 0.0011515750084123918\n",
            "1081 2.5299469036981463 0.0011550688646272352\n",
            "1091 2.466006638860563 0.0011585627208420789\n",
            "1101 3.2884549204609357 0.001162056577056922\n",
            "1111 2.475525868590921 0.0011655504332717655\n",
            "1 3.313761774750674 0.0011662492045147342\n",
            "11 2.531790261156857 0.0011697430607295776\n",
            "21 1.9349684547632933 0.001173236916944421\n",
            "31 2.603403414657805 0.0011767307731592644\n",
            "41 2.6131969245834625 0.0011802246293741079\n",
            "51 1.953728121239692 0.0011837184855889513\n",
            "61 2.595446967286989 0.0011872123418037947\n",
            "71 2.21643816283904 0.0011907061980186383\n",
            "81 2.097818756941706 0.0011942000542334817\n",
            "91 2.8678395478636958 0.0011976939104483251\n",
            "101 2.813362392172621 0.0012011877666631683\n",
            "111 2.0656028226949275 0.0012046816228780117\n",
            "121 2.479992662090808 0.0012081754790928551\n",
            "131 2.5474592486862093 0.0012116693353076988\n",
            "141 2.9144414094553213 0.0012151631915225422\n",
            "151 3.460734924281496 0.0012186570477373856\n",
            "161 2.3473417081404477 0.001222150903952229\n",
            "171 2.7509726629941724 0.0012256447601670724\n",
            "181 3.0303411726054037 0.0012291386163819158\n",
            "191 2.8399126508593326 0.0012326324725967593\n",
            "201 2.3636714017484337 0.0012361263288116027\n",
            "211 2.525929572118912 0.001239620185026446\n",
            "221 2.5802545537007973 0.0012431140412412895\n",
            "231 2.4301193356513977 0.001246607897456133\n",
            "241 1.7994150407612324 0.0012501017536709763\n",
            "251 2.4475135765969753 0.0012535956098858197\n",
            "261 2.5890884117688984 0.0012570894661006631\n",
            "271 2.2372528936248273 0.0012605833223155065\n",
            "281 1.5256257518194616 0.00126407717853035\n",
            "291 2.3023629221133888 0.0012675710347451934\n",
            "301 3.239165122567101 0.0012710648909600368\n",
            "311 2.4453414811287075 0.0012745587471748804\n",
            "321 1.8224271861836314 0.0012780526033897238\n",
            "331 1.8307352499105036 0.0012815464596045672\n",
            "341 2.653455405830755 0.0012850403158194104\n",
            "351 2.49539710348472 0.0012885341720342538\n",
            "361 1.3155895583331585 0.0012920280282490975\n",
            "371 2.381059111794457 0.0012955218844639409\n",
            "381 2.3350926977582276 0.0012990157406787843\n",
            "391 2.006536667700857 0.0013025095968936277\n",
            "401 1.8072619969025254 0.0013060034531084711\n",
            "411 1.8105832189321518 0.0013094973093233145\n",
            "421 2.8130020713433623 0.001312991165538158\n",
            "431 2.0869696573354304 0.0013164850217530014\n",
            "441 2.3514620141650084 0.0013199788779678448\n",
            "451 2.753128018695861 0.0013234727341826882\n",
            "461 2.183602938428521 0.0013269665903975316\n",
            "471 3.334167318575055 0.001330460446612375\n",
            "481 2.973874620569404 0.0013339543028272184\n",
            "491 3.0529768957931083 0.0013374481590420618\n",
            "501 2.392678847303614 0.0013409420152569052\n",
            "511 2.061464569531381 0.0013444358714717487\n",
            "521 2.658319039735943 0.001347929727686592\n",
            "531 2.5663636478129774 0.0013514235839014355\n",
            "541 3.2905926094081224 0.0013549174401162791\n",
            "551 2.3375071426853538 0.0013584112963311225\n",
            "561 2.4391574929468334 0.001361905152545966\n",
            "571 1.8980723330751061 0.0013653990087608091\n",
            "581 2.4699657324235886 0.0013688928649756525\n",
            "591 2.773394554213155 0.001372386721190496\n",
            "601 2.5588202712096972 0.0013758805774053396\n",
            "611 2.8029129919596016 0.001379374433620183\n",
            "621 1.7359274146147072 0.0013828682898350264\n",
            "631 3.470416575011768 0.0013863621460498698\n",
            "641 2.3460161834955215 0.0013898560022647132\n",
            "651 2.4980412463410175 0.0013933498584795566\n",
            "661 1.8586668497882783 0.0013968437146944\n",
            "671 2.4889321324881166 0.001396147036277407\n",
            "681 2.8433373072184622 0.0013944085882911256\n",
            "691 2.88697013125784 0.0013926766181924578\n",
            "701 2.671739791985601 0.0013909510858507287\n",
            "711 2.5544417328201234 0.0013892319514824595\n",
            "721 1.902949367184192 0.001387519175647512\n",
            "731 2.624481431208551 0.0013858127192452904\n",
            "741 2.7584300383459777 0.0013841125435109904\n",
            "751 2.4998105787672102 0.0013824186100119028\n",
            "761 2.506678891069896 0.0013807308806437623\n",
            "771 2.930199123569764 0.0013790493176271497\n",
            "781 2.5937816300429404 0.0013773738835039374\n",
            "791 3.0456781159491584 0.0013757045411337872\n",
            "801 2.0389878880232573 0.0013740412536906912\n",
            "811 2.6894629383750726 0.001372383984659559\n",
            "821 3.178935458359774 0.0013707326978328522\n",
            "831 1.8790131583809853 0.001369087357307261\n",
            "841 2.398347785230726 0.0013674479274804262\n",
            "851 1.9300387995317578 0.0013658143730477018\n",
            "861 1.8597695100324927 0.0013641866589989649\n",
            "871 1.8775691520422697 0.0013625647506154605\n",
            "881 2.677628430246841 0.0013609486134666944\n",
            "891 2.6333874966949224 0.0013593382134073615\n",
            "901 1.8613699483685195 0.0013577335165743164\n",
            "911 2.682608663337305 0.001356134489383582\n",
            "921 1.681377125903964 0.0013545410985273988\n",
            "931 3.0174070133361965 0.0013529533109713095\n",
            "941 2.1117292053531855 0.0013513710939512833\n",
            "951 2.567193202674389 0.0013497944149708752\n",
            "961 2.379631175659597 0.0013482232417984248\n",
            "971 1.5179805574007332 0.0013466575424642866\n",
            "981 2.551912195631303 0.001345097285258099\n",
            "991 2.8752173129469156 0.0013435424387260868\n",
            "1001 1.6689061606302857 0.0013419929716683976\n",
            "1011 2.2965968577191234 0.0013404488531364729\n",
            "1021 2.292700050631538 0.0013389100524304506\n",
            "1031 2.2656006114557385 0.0013373765390966028\n",
            "1041 2.758994953036563 0.0013358482829248042\n",
            "1051 2.9149757593258983 0.0013343252539460312\n",
            "1061 2.7406875882297754 0.001332807422429895\n",
            "1071 3.3504420408225997 0.0013312947588822032\n",
            "1081 2.186062278226018 0.0013297872340425534\n",
            "1091 2.1588013174477965 0.001328284818881955\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_HyZJUAIWGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, batch in enumerate(valid_iter):\n",
        "    src = batch.src.transpose(0, 1)[:1]\n",
        "    src_mask = (src != SRC.vocab.stoi[\"<blank>\"]).unsqueeze(-2)\n",
        "    out = greedy_decode(model, src, src_mask, \n",
        "                        max_len=60, start_symbol=TGT.vocab.stoi[\"<s>\"])\n",
        "    print(\"Translation:\", end=\"\\t\")\n",
        "    for i in range(1, out.size(1)):\n",
        "        sym = TGT.vocab.itos[out[0, i]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    print(\"Target:\", end=\"\\t\")\n",
        "    for i in range(1, batch.trg.size(0)):\n",
        "        sym = TGT.vocab.itos[batch.trg.data[i, 0]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}